import pandas as pd
import numpy as np
import sys
from common.file_utils import ensure_dir
from pathlib import Path
from io import StringIO
from datetime import datetime
from common.path_utils import PHASE3_PREPARE_DIR, PHASE4_MODELS_DIR

# ===================================================================
# å…¨åŸŸè¨­å®š
# ===================================================================
# æ™‚é–“æˆ³è¨˜
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
output_model_dir = Path(PHASE4_MODELS_DIR) / "M3" / f"M3_{timestamp}"
output_prepare_dir = Path(PHASE4_MODELS_DIR) / "M3" / f"M3_{timestamp}" / "prepared_data"
log_file = output_model_dir / f"training_log_{timestamp}.txt"

# å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
ensure_dir(output_prepare_dir)
ensure_dir(output_model_dir)

# ä½¿ç”¨çš„è¨“ç·´è³‡æ–™é›†
input_data_path = Path(PHASE3_PREPARE_DIR) / "M3_train_dataset" / "features_market_2025-11-07.csv"


# ===================================================================
# æ—¥èªŒç³»çµ±è¨­å®š
# ===================================================================
# å»ºç«‹ log ç·©è¡å€
log_buffer = StringIO()


# å»ºç«‹è‡ªè¨‚çš„ print å‡½æ•¸,åŒæ™‚è¼¸å‡ºåˆ°çµ‚ç«¯æ©Ÿå’Œ log
class Logger:
    def __init__(self, log_buffer):
        self.terminal = sys.stdout
        self.log = log_buffer

    def write(self, message):
        # è™•ç† Windows çµ‚ç«¯æ©Ÿç·¨ç¢¼å•é¡Œ
        try:
            self.terminal.write(message)
        except UnicodeEncodeError:
            # ç§»é™¤ç„¡æ³•ç·¨ç¢¼çš„å­—ç¬¦
            clean_message = message.encode("ascii", "ignore").decode("ascii")
            self.terminal.write(clean_message)
        self.log.write(message)

    def flush(self):
        self.terminal.flush()


# é‡å®šå‘ stdout
sys.stdout = Logger(log_buffer)

print("=" * 60)
print(f"ğŸš€ æ¨¡å‹è¨“ç·´é–‹å§‹ (M3 - æ’é™¤å¤§ç‰‡æ¸¬è©¦): {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 60)


# ===================================================================
# è³‡æ–™é è™•ç†
# ===================================================================
# === 1. è®€å–è³‡æ–™ ===
df = pd.read_csv(
    "data/ML_boxoffice/phase2_features/with_market/features_market_2025-11-07.csv"
)  # æ›¿æ›æˆä½ çš„æª”æ¡ˆè·¯å¾‘


# === 2-1. æ’é™¤æŒ‡å®šçš„é›»å½± ===
# æ’é™¤æ¸…å–®è·¯å¾‘
exclude_config_path = "config/exclude_movies.csv"

try:
    exclude_df = pd.read_csv(exclude_config_path, comment="#")
    exclude_gov_ids = exclude_df["gov_id"].dropna().astype(int).tolist()

    if len(exclude_gov_ids) > 0:
        print(f"\nå¾ {exclude_config_path} è®€å–æ’é™¤æ¸…å–®:")
        print(f"  ç™¼ç¾ {len(exclude_gov_ids)} éƒ¨éœ€è¦æ’é™¤çš„é›»å½±")
        print(f"  æ’é™¤çš„ gov_id: {exclude_gov_ids}")

        # æª¢æŸ¥æœ‰å¤šå°‘ç­†è³‡æ–™æœƒè¢«æ’é™¤
        exclude_count = df[df["gov_id"].isin(exclude_gov_ids)].shape[0]
        exclude_movie_count = df[df["gov_id"].isin(exclude_gov_ids)]["gov_id"].nunique()
        print(f"  å°‡æ’é™¤ {exclude_movie_count} éƒ¨é›»å½±ï¼Œå…± {exclude_count} ç­†è³‡æ–™")

        # åŸ·è¡Œæ’é™¤
        df = df[~df["gov_id"].isin(exclude_gov_ids)].copy()
        print(f"  æ’é™¤å¾Œå‰©é¤˜è³‡æ–™ç­†æ•¸: {len(df)}")
    else:
        print(f"\n{exclude_config_path} ä¸­æ²’æœ‰éœ€è¦æ’é™¤çš„é›»å½±")

except FileNotFoundError:
    print(f"\nè­¦å‘Š: æ‰¾ä¸åˆ°æ’é™¤æ¸…å–®æª”æ¡ˆ {exclude_config_path}ï¼Œè·³éæ’é™¤æ­¥é©Ÿ")
except Exception as e:
    print(f"\nè­¦å‘Š: è®€å–æ’é™¤æ¸…å–®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ï¼Œè·³éæ’é™¤æ­¥é©Ÿ")


# === 2-2. ç¯©é¸è³‡æ–™ ===
# åªä¿ç•™é¦–è¼ªè³‡æ–™
df = df[df["round_idx"] == 1].copy()
# åªä¿ç•™æœ‰æ´»èºé€±æ¬¡çš„è³‡æ–™
df = df[df["current_week_active_idx"].notna()]
# å¿…é ˆåŒæ™‚æœ‰ week_1 å’Œ week_2 çš„è³‡æ–™,ä¸”éƒ½ä¸ç‚º 0
df = df[
    (df["boxoffice_week_1"].notna())
    & (df["boxoffice_week_1"] > 0)
    & (df["boxoffice_week_2"].notna())
    & (df["boxoffice_week_2"] > 0)
]
print(f"åŸºæœ¬ç¯©é¸å¾Œè³‡æ–™ç­†æ•¸: {len(df)}")


# === 3. æœˆä»½é€±æœŸæ€§ç·¨ç¢¼ ===
df["release_month_sin"] = np.sin(2 * np.pi * df["release_month"] / 12)
df["release_month_cos"] = np.cos(2 * np.pi * df["release_month"] / 12)


# === 4. åˆªé™¤ä¸éœ€è¦çš„æ¬„ä½ ===
# å®šç¾©è¦åˆªé™¤çš„æ¬„ä½
drop_columns = [
    # è³‡æ–™æ´©æ¼
    "tickets",
    "theater_count",  # amount è¦ç•™åˆ°æœ€å¾Œæ‰åˆª
    # ä¸éœ€è¦çš„æ™‚é–“è³‡è¨Š
    "official_release_date",
    "week_range",
    "current_week_real_idx",
    # è·¨è¼ªç´¯ç©
    "boxoffice_cumsum",
    "boxoffice_round1_cumsum",
    "boxoffice_current_round_cumsum",  # â† æª¢æŸ¥é€™å€‹
    "audience_cumsum",
    "audience_round1_cumsum",
    "audience_current_round_cumsum",  # â† æª¢æŸ¥é€™å€‹
    "rounds_cumsum",
    # å•é¡Œæ¬„ä½
    "ticket_price_avg_current",
    # åˆ†é¡æ¬„ä½ (æ™‚é–“æœ‰é™å…ˆåˆªé™¤)
    "region",
    "publisher",
    # å·²ç·¨ç¢¼çš„åŸå§‹æ¬„ä½
    "release_month",
]

df = df.drop(columns=drop_columns)


# === 5. æª¢æŸ¥ç¼ºå¤±å€¼ ===
print("\n=== ç¼ºå¤±å€¼æª¢æŸ¥ ===")
print(df.isnull().sum()[df.isnull().sum() > 0])


# === 6. å­˜æª”: å®Œæ•´è³‡æ–™ (å« amount å’Œ gov_id) ===
df.to_csv(output_prepare_dir / "preprocessed_full.csv", index=False, encoding="utf-8-sig")
print(f"\nâœ… å·²å­˜æª”: {output_prepare_dir / 'preprocessed_full.csv'}")
print("ğŸ“è³‡æ–™æ•¸é‡å°è¨ˆ:")
print(f"   æ¬„ä½æ•¸: {len(df.columns)}")
print(f"   è³‡æ–™ç­†æ•¸: {len(df)}")


# === 7. é¡¯ç¤ºæœ€çµ‚æ¬„ä½ ===
print("\n=== ğŸ“æœ€çµ‚æ¬„ä½æ¸…å–® ===")
for i, col in enumerate(df.columns, 1):
    print(f"{i:2d}. {col}")


# === 8. å­˜æª”: è¨“ç·´ç”¨ç‰¹å¾µ (ç§»é™¤ amountï¼Œä¿ç•™ gov_id ç”¨æ–¼åˆ†çµ„) ===
feature_cols = [col for col in df.columns if col != "amount"]
df_features = df[feature_cols]
df_features.to_csv(
    output_prepare_dir / "preprocessed_features.csv", index=False, encoding="utf-8-sig"
)
print(f"\nâœ… å·²å­˜æª”: {output_prepare_dir / 'preprocessed_features.csv'}")


# === 9. å­˜æª”: ç›®æ¨™è®Šæ•¸ ===
df[["gov_id", "amount"]].to_csv(
    output_prepare_dir / "preprocessed_target.csv", index=False, encoding="utf-8-sig"
)
print(f"âœ… å·²å­˜æª”: {output_prepare_dir / 'preprocessed_target.csv'}")


# === 10. çµ±è¨ˆæ‘˜è¦ ===
print("\n=== ğŸ“è³‡æ–™æ‘˜è¦ ===")
print(df[["amount", "boxoffice_week_1", "current_week_active_idx"]].describe())


# ===================================================================
# è¨“ç·´æ¨¡å‹
# ===================================================================
# === 11. åˆ†é›¢ç‰¹å¾µèˆ‡ç›®æ¨™ ===
X = df.drop(columns=["amount"])
y = df["amount"]

print(f"\nç‰¹å¾µçŸ©é™£ X: {X.shape}")
print(f"ç›®æ¨™è®Šæ•¸ y: {y.shape}")

print("\n" + "=" * 50)
print("ğŸ” ç‰¹å¾µèˆ‡ç›®æ¨™ç›¸é—œæ€§ (Top 10)")
print("=" * 50)

correlation = pd.DataFrame(
    {
        "feature": X.drop(columns=["gov_id"]).columns,
        "correlation": X.drop(columns=["gov_id"]).corrwith(y),
    }
).sort_values("correlation", key=abs, ascending=False)

print(correlation.head(10).to_string(index=False))


# === 12. Group-based åˆ‡åˆ†è³‡æ–™é›† ===
from sklearn.model_selection import GroupShuffleSplit

# ç¢ºä¿åŒä¸€éƒ¨é›»å½±çš„æ‰€æœ‰é€±æ¬¡è³‡æ–™ä¸æœƒåŒæ™‚å‡ºç¾åœ¨è¨“ç·´/æ¸¬è©¦é›†
splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)
train_idx, test_idx = next(splitter.split(X, y, groups=X["gov_id"]))

X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

print("\nğŸ“ è¨“ç·´é›†/æ¸¬è©¦é›†åˆ†å‰²çµæœ (åŸå§‹)")
print(f"è¨“ç·´é›†: {len(X_train)} ç­† ({len(X_train['gov_id'].unique())} éƒ¨é›»å½±)")
print(f"æ¸¬è©¦é›†: {len(X_test)} ç­† ({len(X_test['gov_id'].unique())} éƒ¨é›»å½±)")


# === 13. [M3 ç‰¹è‰²] åœ¨æ¸¬è©¦é›†ä¸­æ’é™¤å¤§ç‰‡ ===
print("\n" + "=" * 60)
print("ğŸ¬ [M3 ç‰¹è‰²] æ¸¬è©¦é›†æ’é™¤å¤§ç‰‡æ¨£æœ¬")
print("=" * 60)

# ===================================================================
# éæ¿¾è¨­å®šï¼ˆå¯å½ˆæ€§èª¿æ•´ï¼‰
# ===================================================================

# 1ï¸âƒ£ é¸æ“‡éæ¿¾åŸºæº–æ¬„ä½
FILTER_COLUMN = "amount"  # é¸é …: "amount", "boxoffice_week_1", "boxoffice_week_2" ç­‰

# 2ï¸âƒ£ é¸æ“‡ç™¾åˆ†ä½æ•¸é–¾å€¼ï¼ˆæ’é™¤ã€Œé«˜æ–¼ã€æ­¤ç™¾åˆ†ä½æ•¸çš„æ¨£æœ¬ï¼‰
#    ä¾‹å¦‚: 85 è¡¨ç¤ºæ’é™¤å‰ 15% (ä¿ç•™å¾Œ85%)
#          90 è¡¨ç¤ºæ’é™¤å‰ 10% (ä¿ç•™å¾Œ90%)
#          95 è¡¨ç¤ºæ’é™¤å‰ 5%  (ä¿ç•™å¾Œ95%)
PERCENTILE_THRESHOLD = 95

# 3ï¸âƒ£ æ˜¯å¦åˆªé™¤æ•´éƒ¨é›»å½±ï¼ˆTrue: åˆªé™¤æ•´éƒ¨é›»å½±çš„æ‰€æœ‰é€±æ¬¡ï¼ŒFalse: åªåˆªé™¤ç¬¦åˆæ¢ä»¶çš„æ¨£æœ¬ï¼‰
EXCLUDE_WHOLE_MOVIE = True

# 4ï¸âƒ£ æˆ–ä½¿ç”¨çµ•å°é–¾å€¼ï¼ˆå–æ¶ˆè¨»è§£ä»¥ä½¿ç”¨ï¼Œæœƒè¦†è“‹ç™¾åˆ†ä½æ•¸è¨­å®šï¼‰
# ABSOLUTE_THRESHOLD = 10_000_000  # æ’é™¤ > 1000è¬çš„æ¨£æœ¬
# FILTER_COLUMN = "amount"

# ===================================================================

# è¨ˆç®—éæ¿¾åŸºæº–å€¼
if FILTER_COLUMN == "amount":
    filter_values = y_test
else:
    filter_values = X_test[FILTER_COLUMN]

# åˆ¤æ–·ä½¿ç”¨ç™¾åˆ†ä½æ•¸é‚„æ˜¯çµ•å°å€¼
if "ABSOLUTE_THRESHOLD" in locals():
    threshold = ABSOLUTE_THRESHOLD
    threshold_method = f"çµ•å°å€¼ > {threshold:,.0f}"
else:
    threshold = np.percentile(filter_values, PERCENTILE_THRESHOLD)
    threshold_method = f"ç¬¬ {PERCENTILE_THRESHOLD} ç™¾åˆ†ä½æ•¸ (æ’é™¤å‰ {100-PERCENTILE_THRESHOLD}%)"

print(f"\néæ¿¾è¨­å®š:")
print(f"  åŸºæº–æ¬„ä½: {FILTER_COLUMN}")
print(f"  é–¾å€¼: {threshold:,.0f} ({threshold_method})")
print(f"  åˆªé™¤æ•´éƒ¨é›»å½±: {'æ˜¯' if EXCLUDE_WHOLE_MOVIE else 'å¦ï¼ˆåƒ…åˆªé™¤å–®ç­†æ¨£æœ¬ï¼‰'}")

# æ‰¾å‡ºè¦æ’é™¤çš„æ¨£æœ¬
blockbuster_mask = filter_values > threshold

if EXCLUDE_WHOLE_MOVIE:
    # æ‰¾å‡ºæ‰€æœ‰è¦æ’é™¤çš„é›»å½± gov_id
    blockbuster_gov_ids = X_test.loc[blockbuster_mask, "gov_id"].unique()
    # å°‡æ•´éƒ¨é›»å½±çš„æ‰€æœ‰æ¨£æœ¬æ¨™è¨˜ç‚ºè¦æ’é™¤
    blockbuster_mask_full = X_test["gov_id"].isin(blockbuster_gov_ids)
    blockbuster_count = blockbuster_mask_full.sum()

    print(f"\næ¸¬è©¦é›†ä¸­è¦æ’é™¤çš„é›»å½±:")
    print(f"  é›»å½±æ•¸: {len(blockbuster_gov_ids)} éƒ¨")
    print(f"  æ¨£æœ¬æ•¸: {blockbuster_count} ç­†ï¼ˆåŒ…å«è©²é›»å½±çš„æ‰€æœ‰é€±æ¬¡ï¼‰")
    print(f"  gov_id: {sorted(blockbuster_gov_ids.tolist())}")

    # ä½¿ç”¨å®Œæ•´çš„é®ç½©
    final_mask = ~blockbuster_mask_full
    excluded_gov_ids = blockbuster_gov_ids  # ç”¨æ–¼å¾ŒçºŒå„²å­˜
else:
    blockbuster_count = blockbuster_mask.sum()
    blockbuster_gov_ids = X_test.loc[blockbuster_mask, "gov_id"].unique()

    print(f"\næ¸¬è©¦é›†ä¸­è¦æ’é™¤çš„æ¨£æœ¬:")
    print(f"  æ¨£æœ¬æ•¸: {blockbuster_count} ç­†")
    print(f"  æ¶‰åŠé›»å½±: {len(blockbuster_gov_ids)} éƒ¨")
    print(f"  gov_id: {sorted(blockbuster_gov_ids.tolist())}")

    final_mask = ~blockbuster_mask
    excluded_gov_ids = blockbuster_gov_ids  # ç”¨æ–¼å¾ŒçºŒå„²å­˜

# éæ¿¾æ¸¬è©¦é›†
X_test_filtered = X_test[final_mask].copy()
y_test_filtered = y_test[final_mask].copy()

print(f"\néæ¿¾å¾Œçš„æ¸¬è©¦é›†:")
print(
    f"  æ¨£æœ¬æ•¸: {len(X_test_filtered)} ç­† (åŸå§‹: {len(X_test)}, æ¸›å°‘ {len(X_test) - len(X_test_filtered)} ç­†)"
)
print(
    f"  é›»å½±æ•¸: {len(X_test_filtered['gov_id'].unique())} éƒ¨ (åŸå§‹: {len(X_test['gov_id'].unique())})"
)
print(f"  ä¿ç•™æ¯”ä¾‹: {len(X_test_filtered)/len(X_test)*100:.1f}%")

# âš ï¸ æª¢æŸ¥æ¸¬è©¦é›†æ˜¯å¦å¤ªå°
if len(X_test_filtered) < 30:
    print(f"\nâš ï¸ è­¦å‘Š: æ¸¬è©¦é›†æ¨£æœ¬æ•¸éå°‘ ({len(X_test_filtered)} ç­†)ï¼Œæ¨¡å‹è©•ä¼°å¯èƒ½ä¸ç©©å®šï¼")
    print(f"   å»ºè­°: èª¿æ•´ PERCENTILE_THRESHOLD æˆ–è¨­å®š EXCLUDE_WHOLE_MOVIE = False")
elif len(X_test_filtered) < len(X_test) * 0.3:
    print(f"\nâš ï¸ æ³¨æ„: æ¸¬è©¦é›†å·²æ¸›å°‘è¶…é 70%ï¼Œè«‹ç•™æ„è©•ä¼°çµæœçš„ä»£è¡¨æ€§")

# æª¢æŸ¥éæ¿¾å¾Œçš„è³‡æ–™åˆ†å¸ƒ
print(f"\néæ¿¾å¾Œçš„ {FILTER_COLUMN} åˆ†å¸ƒ:")
if FILTER_COLUMN == "amount":
    print(f"  æœ€å°å€¼: {y_test_filtered.min():,.0f}")
    print(f"  æœ€å¤§å€¼: {y_test_filtered.max():,.0f}")
    print(f"  å¹³å‡å€¼: {y_test_filtered.mean():,.0f}")
    print(f"  ä¸­ä½æ•¸: {y_test_filtered.median():,.0f}")
else:
    print(f"  æœ€å°å€¼: {X_test_filtered[FILTER_COLUMN].min():,.0f}")
    print(f"  æœ€å¤§å€¼: {X_test_filtered[FILTER_COLUMN].max():,.0f}")
    print(f"  å¹³å‡å€¼: {X_test_filtered[FILTER_COLUMN].mean():,.0f}")

# åŒæ™‚ä¿ç•™åŸå§‹æ¸¬è©¦é›†ç”¨æ–¼å°æ¯”
X_test_original = X_test.copy()
y_test_original = y_test.copy()


# === 14. ç§»é™¤ gov_id (åªç”¨æ–¼åˆ†çµ„,ä¸åƒèˆ‡è¨“ç·´) ===
X_train_model = X_train.drop(columns=["gov_id"])
X_test_model_filtered = X_test_filtered.drop(columns=["gov_id"])

print(f"\næ¨¡å‹è¨“ç·´ç‰¹å¾µæ•¸: {X_train_model.shape[1]}")


# === 15. æª¢æŸ¥ç¼ºå¤±å€¼ ===
print("\n" + "=" * 50)
print("ğŸ” è¨“ç·´é›†ç¼ºå¤±å€¼æª¢æŸ¥")
print("=" * 50)

missing_train = X_train_model.isnull().sum()
missing_train = missing_train[missing_train > 0].sort_values(ascending=False)

if len(missing_train) > 0:
    print("âš ï¸ ç™¼ç¾ç¼ºå¤±å€¼:")
    print(missing_train)
    print(f"\nç¸½ç¼ºå¤±ç­†æ•¸: {X_train_model.isnull().any(axis=1).sum()}/{len(X_train_model)}")
else:
    print("âœ… ç„¡ç¼ºå¤±å€¼")


# === 16. è¨“ç·´æ¨¡å‹ 1: Linear Regression ===
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

print("\n" + "=" * 50)
print("ğŸ”µ æ¨¡å‹ 1: Linear Regression")
print("=" * 50)

lr_model = LinearRegression()
lr_model.fit(X_train_model, y_train)

y_pred_lr = lr_model.predict(X_test_model_filtered)

print(f"MAE:  {mean_absolute_error(y_test_filtered, y_pred_lr):,.0f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test_filtered, y_pred_lr)):,.0f}")
print(f"RÂ²:   {r2_score(y_test_filtered, y_pred_lr):.4f}")


# === 17. è¨“ç·´æ¨¡å‹ 2: LightGBM ===
import lightgbm as lgb

print("\n" + "=" * 50)
print("ğŸŸ¢ æ¨¡å‹ 2: LightGBM")
print("=" * 50)

lgb_model = lgb.LGBMRegressor(
    n_estimators=100,
    learning_rate=0.05,
    max_depth=5,
    random_state=42,
    verbose=-1,  # é—œé–‰è¨“ç·´éç¨‹è¼¸å‡º
)

lgb_model.fit(X_train_model, y_train)

y_pred_lgb = lgb_model.predict(X_test_model_filtered)

print(f"MAE:  {mean_absolute_error(y_test_filtered, y_pred_lgb):,.0f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test_filtered, y_pred_lgb)):,.0f}")
print(f"RÂ²:   {r2_score(y_test_filtered, y_pred_lgb):.4f}")


# === 18. è¨“ç·´æ¨¡å‹ 3: Decision Tree Regressor ===
from sklearn.tree import DecisionTreeRegressor

print("\n" + "=" * 50)
print("ğŸŸ¡ æ¨¡å‹ 3: Decision Tree Regressor")
print("=" * 50)

dt_model = DecisionTreeRegressor(
    max_depth=10,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42,
)

dt_model.fit(X_train_model, y_train)

y_pred_dt = dt_model.predict(X_test_model_filtered)

print(f"MAE:  {mean_absolute_error(y_test_filtered, y_pred_dt):,.0f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test_filtered, y_pred_dt)):,.0f}")
print(f"RÂ²:   {r2_score(y_test_filtered, y_pred_dt):.4f}")


# ===================================================================
# æ´å¯Ÿæ¨¡å‹åˆ†æçµæœ
# ===================================================================
# === 19. ç‰¹å¾µé‡è¦æ€§åˆ†æ ===
print("\n" + "=" * 50)
print("ğŸ“Š Top 10 é‡è¦ç‰¹å¾µ (LightGBM)")
print("=" * 50)

feature_importance_lgb = pd.DataFrame(
    {"feature": X_train_model.columns, "importance": lgb_model.feature_importances_}
).sort_values("importance", ascending=False)

print(feature_importance_lgb.head(10).to_string(index=False))

# å­˜æª”ç‰¹å¾µé‡è¦æ€§
feature_importance_lgb.to_csv(
    output_model_dir / "feature_importance_lgb.csv", index=False, encoding="utf-8-sig"
)
print(f"\nâœ… LightGBMç‰¹å¾µé‡è¦æ€§å·²å­˜æª”: {output_model_dir / 'feature_importance_lgb.csv'}")

print("\n" + "=" * 50)
print("ğŸ“Š Top 10 é‡è¦ç‰¹å¾µ (Decision Tree)")
print("=" * 50)

feature_importance_dt = pd.DataFrame(
    {"feature": X_train_model.columns, "importance": dt_model.feature_importances_}
).sort_values("importance", ascending=False)

print(feature_importance_dt.head(10).to_string(index=False))

feature_importance_dt.to_csv(
    output_model_dir / "feature_importance_dt.csv", index=False, encoding="utf-8-sig"
)
print(f"\nâœ… Decision Treeç‰¹å¾µé‡è¦æ€§å·²å­˜æª”: {output_model_dir / 'feature_importance_dt.csv'}")


# === 20. è¦–è¦ºåŒ–: é æ¸¬ vs å¯¦éš› (ä¸‰æ¨¡å‹æ¯”è¼ƒ) ===
import matplotlib.pyplot as plt

plt.rcParams["font.sans-serif"] = ["Microsoft JhengHei"]  # ä¸­æ–‡å­—å‹
plt.rcParams["axes.unicode_minus"] = False

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Linear Regression
axes[0].scatter(y_test_filtered, y_pred_lr, alpha=0.5, s=10)
axes[0].plot(
    [y_test_filtered.min(), y_test_filtered.max()],
    [y_test_filtered.min(), y_test_filtered.max()],
    "r--",
    lw=2,
)
axes[0].set_xlabel("å¯¦éš›ç¥¨æˆ¿")
axes[0].set_ylabel("é æ¸¬ç¥¨æˆ¿")
axes[0].set_title(f"Linear Regression (RÂ²={r2_score(y_test_filtered, y_pred_lr):.3f})")
axes[0].grid(True, alpha=0.3)

# LightGBM
axes[1].scatter(y_test_filtered, y_pred_lgb, alpha=0.5, s=10, color="green")
axes[1].plot(
    [y_test_filtered.min(), y_test_filtered.max()],
    [y_test_filtered.min(), y_test_filtered.max()],
    "r--",
    lw=2,
)
axes[1].set_xlabel("å¯¦éš›ç¥¨æˆ¿")
axes[1].set_ylabel("é æ¸¬ç¥¨æˆ¿")
axes[1].set_title(f"LightGBM (RÂ²={r2_score(y_test_filtered, y_pred_lgb):.3f})")
axes[1].grid(True, alpha=0.3)

# Decision Tree
axes[2].scatter(y_test_filtered, y_pred_dt, alpha=0.5, s=10, color="orange")
axes[2].plot(
    [y_test_filtered.min(), y_test_filtered.max()],
    [y_test_filtered.min(), y_test_filtered.max()],
    "r--",
    lw=2,
)
axes[2].set_xlabel("å¯¦éš›ç¥¨æˆ¿")
axes[2].set_ylabel("é æ¸¬ç¥¨æˆ¿")
axes[2].set_title(f"Decision Tree (RÂ²={r2_score(y_test_filtered, y_pred_dt):.3f})")
axes[2].grid(True, alpha=0.3)

plt.suptitle(f"M3 æ¨¡å‹æ¯”è¼ƒ (æ’é™¤ç¥¨æˆ¿å‰ {100-PERCENTILE_THRESHOLD}% å¤§ç‰‡)", fontsize=14, y=1.02)
plt.tight_layout()
plt.savefig(output_model_dir / "prediction_comparison.png", dpi=150, bbox_inches="tight")
print(f"\nâœ… é æ¸¬çµæœåœ–å·²å­˜æª”: {output_model_dir / 'prediction_comparison.png'}")
plt.show()


# ===================================================================
# å„²å­˜æ¨¡å‹èˆ‡åˆ†æçµæœ
# ===================================================================
# === 21. å„²å­˜æ¨¡å‹ ===
import joblib

joblib.dump(lr_model, output_model_dir / "model_linear_regression.pkl")
joblib.dump(lgb_model, output_model_dir / "model_lightgbm.pkl")
joblib.dump(dt_model, output_model_dir / "model_decision_tree.pkl")
print(f"\nâœ… æ¨¡å‹å·²å­˜æª”:")
print(f"   - {output_model_dir / 'model_linear_regression.pkl'}")
print(f"   - {output_model_dir / 'model_lightgbm.pkl'}")
print(f"   - {output_model_dir / 'model_decision_tree.pkl'}")


# === 22. å„²å­˜æ¸¬è©¦é›†é æ¸¬çµæœ ===
results = pd.DataFrame(
    {
        "gov_id": X_test_filtered["gov_id"].values,
        "actual": y_test_filtered.values,
        "pred_lr": y_pred_lr,
        "pred_lgb": y_pred_lgb,
        "pred_dt": y_pred_dt,
        "error_lr": y_test_filtered.values - y_pred_lr,
        "error_lgb": y_test_filtered.values - y_pred_lgb,
        "error_dt": y_test_filtered.values - y_pred_dt,
    }
)

results.to_csv(output_model_dir / "test_predictions.csv", index=False, encoding="utf-8-sig")
print(f"âœ… æ¸¬è©¦é›†é æ¸¬çµæœå·²å­˜æª”: {output_model_dir / 'test_predictions.csv'}")

# åŒæ™‚å„²å­˜è¢«æ’é™¤çš„å¤§ç‰‡è³‡è¨Š
if blockbuster_count > 0:
    # å„²å­˜è¢«æ’é™¤çš„æ¨£æœ¬è³‡è¨Š
    excluded_mask = X_test_original["gov_id"].isin(excluded_gov_ids)
    blockbuster_info = pd.DataFrame(
        {
            "gov_id": X_test_original.loc[excluded_mask, "gov_id"].values,
            "actual_amount": y_test_original[excluded_mask].values,
        }
    )

    # æŒ‰ gov_id åˆ†çµ„çµ±è¨ˆ
    excluded_summary = (
        blockbuster_info.groupby("gov_id")
        .agg({"actual_amount": ["count", "sum", "mean", "max"]})
        .reset_index()
    )
    excluded_summary.columns = ["gov_id", "æ¨£æœ¬æ•¸", "ç¸½ç¥¨æˆ¿", "å¹³å‡ç¥¨æˆ¿", "æœ€å¤§ç¥¨æˆ¿"]

    blockbuster_info.to_csv(
        output_model_dir / "excluded_samples.csv", index=False, encoding="utf-8-sig"
    )
    excluded_summary.to_csv(
        output_model_dir / "excluded_movies_summary.csv", index=False, encoding="utf-8-sig"
    )
    print(f"âœ… è¢«æ’é™¤çš„æ¨£æœ¬å·²å­˜æª”: {output_model_dir / 'excluded_samples.csv'}")
    print(f"âœ… è¢«æ’é™¤çš„é›»å½±çµ±è¨ˆå·²å­˜æª”: {output_model_dir / 'excluded_movies_summary.csv'}")


# === 23. æ¨¡å‹è¡¨ç¾ç¸½çµ ===
print("\n" + "=" * 60)
print("ğŸ“Š M3 æ¨¡å‹è¡¨ç¾ç¸½çµ (æ’é™¤å¤§ç‰‡å¾Œ)")
print("=" * 60)

summary = pd.DataFrame(
    {
        "æ¨¡å‹": ["Linear Regression", "LightGBM", "Decision Tree"],
        "MAE": [
            mean_absolute_error(y_test_filtered, y_pred_lr),
            mean_absolute_error(y_test_filtered, y_pred_lgb),
            mean_absolute_error(y_test_filtered, y_pred_dt),
        ],
        "RMSE": [
            np.sqrt(mean_squared_error(y_test_filtered, y_pred_lr)),
            np.sqrt(mean_squared_error(y_test_filtered, y_pred_lgb)),
            np.sqrt(mean_squared_error(y_test_filtered, y_pred_dt)),
        ],
        "RÂ²": [
            r2_score(y_test_filtered, y_pred_lr),
            r2_score(y_test_filtered, y_pred_lgb),
            r2_score(y_test_filtered, y_pred_dt),
        ],
    }
)

print(summary.to_string(index=False))
summary.to_csv(output_model_dir / "model_summary.csv", index=False, encoding="utf-8-sig")
print(f"\nâœ… æ¨¡å‹ç¸½çµå·²å­˜æª”: {output_model_dir / 'model_summary.csv'}")


# === 24. ç´€éŒ„æœ¬æ¬¡åŸ·è¡Œéç¨‹log ===
print("\n" + "=" * 60)
print(f"âœ… è¨“ç·´å®Œæˆ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 60)

# å¯«å…¥ log æª”æ¡ˆ
sys.stdout = sys.stdout.terminal  # æ¢å¾©æ­£å¸¸çš„ stdout
with open(log_file, "w", encoding="utf-8") as f:
    f.write(log_buffer.getvalue())

print(f"\nâœ… è¨“ç·´ç´€éŒ„å·²å­˜æª”: {log_file}")
print("\nğŸ‰ è¨“ç·´å®Œæˆ!")

# ===================================================================
#                                 END
# ===================================================================

# ===================================================================
# è£œå……èªªæ˜
# ===================================================================
"""
## ğŸ“¦ M3 æœ€çµ‚æœƒç”¢ç”Ÿçš„æª”æ¡ˆ
```
data/ML_boxoffice/phase3_prepare/M3_YYYYMMDD_HHMMSS/
â”œâ”€â”€ preprocessed_full.csv           # å®Œæ•´é è™•ç†è³‡æ–™
â”œâ”€â”€ preprocessed_features.csv       # ç‰¹å¾µçŸ©é™£ (X)
â””â”€â”€ preprocessed_target.csv         # ç›®æ¨™è®Šæ•¸ (y)

data/ML_boxoffice/phase4_models/M3_YYYYMMDD_HHMMSS/
â”œâ”€â”€ training_log_YYYYMMDD_HHMMSS.txt  # è¨“ç·´æ—¥èªŒ
â”œâ”€â”€ feature_importance_lgb.csv      # LightGBM ç‰¹å¾µé‡è¦æ€§
â”œâ”€â”€ feature_importance_dt.csv       # Decision Tree ç‰¹å¾µé‡è¦æ€§
â”œâ”€â”€ prediction_comparison.png       # ä¸‰æ¨¡å‹æ¯”è¼ƒåœ– (1x3)
â”œâ”€â”€ test_predictions.csv            # æ¸¬è©¦é›†è©³ç´°é æ¸¬çµæœ
â”œâ”€â”€ excluded_blockbusters.csv       # è¢«æ’é™¤çš„å¤§ç‰‡æ¸…å–®
â”œâ”€â”€ model_summary.csv               # ä¸‰æ¨¡å‹è¡¨ç¾ç¸½çµ
â”œâ”€â”€ model_linear_regression.pkl     # å·²è¨“ç·´çš„ LR æ¨¡å‹
â”œâ”€â”€ model_lightgbm.pkl              # å·²è¨“ç·´çš„ LightGBM æ¨¡å‹
â””â”€â”€ model_decision_tree.pkl         # å·²è¨“ç·´çš„ DT æ¨¡å‹
```

## ğŸ¯ M3 çš„ç‰¹è‰²
1. **è¨“ç·´é›†ä¸è®Š**ï¼šä½¿ç”¨èˆ‡ M1/M2 ç›¸åŒçš„è¨“ç·´è³‡æ–™
2. **æ¸¬è©¦é›†å½ˆæ€§éæ¿¾**ï¼šå¯è‡ªè¨‚éæ¿¾æ¢ä»¶
3. **ä¸‰æ¨¡å‹æ¯”è¼ƒ**ï¼šLinear Regressionã€LightGBMã€Decision Tree
4. **é›¢ç¾¤å€¼åˆ†æ**ï¼šè§€å¯Ÿæ’é™¤å¤§ç‰‡å¾Œï¼Œé›¢ç¾¤å€¼æ˜¯å¦æ¶ˆå¤±

## ğŸ”§ å½ˆæ€§éæ¿¾æ§åˆ¶

### 1ï¸âƒ£ é¸æ“‡éæ¿¾åŸºæº–æ¬„ä½
```python
FILTER_COLUMN = "amount"  # å¯é¸: "amount", "boxoffice_week_1", "boxoffice_week_2" ç­‰
```

### 2ï¸âƒ£ é¸æ“‡ç™¾åˆ†ä½æ•¸é–¾å€¼ï¼ˆæ’é™¤ã€Œé«˜æ–¼ã€æ­¤ç™¾åˆ†ä½æ•¸çš„æ¨£æœ¬ï¼‰
```python
PERCENTILE_THRESHOLD = 90  # æ’é™¤å‰ 10% (ä¿ç•™å¾Œ90%)
PERCENTILE_THRESHOLD = 85  # æ’é™¤å‰ 15% (ä¿ç•™å¾Œ85%)
PERCENTILE_THRESHOLD = 95  # æ’é™¤å‰ 5%  (ä¿ç•™å¾Œ95%)
```

### 3ï¸âƒ£ æ˜¯å¦åˆªé™¤æ•´éƒ¨é›»å½±
```python
EXCLUDE_WHOLE_MOVIE = True   # åˆªé™¤æ•´éƒ¨é›»å½±çš„æ‰€æœ‰é€±æ¬¡
EXCLUDE_WHOLE_MOVIE = False  # åªåˆªé™¤ç¬¦åˆæ¢ä»¶çš„å–®ç­†æ¨£æœ¬
```

### 4ï¸âƒ£ æˆ–ä½¿ç”¨çµ•å°é–¾å€¼
```python
ABSOLUTE_THRESHOLD = 10_000_000  # æ’é™¤ > 1000è¬çš„æ¨£æœ¬ï¼ˆå–æ¶ˆè¨»è§£ä»¥ä½¿ç”¨ï¼‰
```

## âš ï¸ å®‰å…¨æ©Ÿåˆ¶
- è‡ªå‹•æª¢æ¸¬æ¸¬è©¦é›†æ¨£æœ¬æ•¸ï¼Œéå°‘æ™‚æœƒç™¼å‡ºè­¦å‘Š
- é¡¯ç¤ºè©³ç´°çš„éæ¿¾å‰å¾Œçµ±è¨ˆè³‡è¨Š
- å„²å­˜è¢«æ’é™¤é›»å½±çš„è©³ç´°è³‡è¨Š
"""
