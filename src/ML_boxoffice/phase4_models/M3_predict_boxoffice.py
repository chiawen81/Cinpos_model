import sys
from pathlib import Path

# å°‡ src ç›®éŒ„åŠ å…¥ Python è·¯å¾‘ï¼Œä»¥ä¾¿èƒ½å¤  import common æ¨¡çµ„
project_root = Path(__file__).resolve().parent.parent.parent.parent
src_path = project_root / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

import pandas as pd
import numpy as np
from common.file_utils import ensure_dir
from io import StringIO
from datetime import datetime
from common.path_utils import PHASE3_PREPARE_DIR, PHASE4_MODELS_DIR
from ML_boxoffice.common.feature_engineering import BoxOfficeFeatureEngineer

# ===================================================================
# å…¨åŸŸè¨­å®š
# ===================================================================
# æ™‚é–“æˆ³è¨˜
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
output_model_dir = Path(PHASE4_MODELS_DIR) / "M3" / f"M3_{timestamp}"
output_prepare_dir = Path(PHASE4_MODELS_DIR) / "M3" / f"M3_{timestamp}" / "prepared_data"
log_file = output_model_dir / f"training_log_{timestamp}.txt"

# å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
ensure_dir(output_prepare_dir)
ensure_dir(output_model_dir)

# ä½¿ç”¨çš„è¨“ç·´è³‡æ–™é›†
input_data_path = Path(PHASE3_PREPARE_DIR) / "M3_train_dataset" / "features_market_2025-11-07.csv"

# ===================================================================
# æ—¥èªŒç³»çµ±è¨­å®š
# ===================================================================
# å»ºç«‹ log ç·©è¡å€
log_buffer = StringIO()


# å»ºç«‹è‡ªè¨‚çš„ print å‡½æ•¸,åŒæ™‚è¼¸å‡ºåˆ°çµ‚ç«¯æ©Ÿå’Œ log
class Logger:
    def __init__(self, log_buffer):
        self.terminal = sys.stdout
        self.log = log_buffer

    def write(self, message):
        # è™•ç† Windows çµ‚ç«¯æ©Ÿç·¨ç¢¼å•é¡Œ
        try:
            self.terminal.write(message)
        except UnicodeEncodeError:
            # ç§»é™¤ç„¡æ³•ç·¨ç¢¼çš„å­—ç¬¦
            clean_message = message.encode("ascii", "ignore").decode("ascii")
            self.terminal.write(clean_message)
        self.log.write(message)

    def flush(self):
        self.terminal.flush()


# é‡å®šå‘ stdout
sys.stdout = Logger(log_buffer)

print("=" * 60)
print(f"ğŸš€ æ¨¡å‹è¨“ç·´é–‹å§‹ (M3 - Decision Tree): {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 60)


# ===================================================================
# è³‡æ–™é è™•ç†
# ===================================================================
# === 1. è®€å–è³‡æ–™ ===
df = pd.read_csv(input_data_path)


# === 2-1. æ’é™¤æŒ‡å®šçš„é›»å½± ===
# æ’é™¤æ¸…å–®è·¯å¾‘
exclude_config_path = "config/exclude_movies.csv"

try:
    exclude_df = pd.read_csv(exclude_config_path, comment="#")
    exclude_gov_ids = exclude_df["gov_id"].dropna().astype(int).tolist()

    if len(exclude_gov_ids) > 0:
        print(f"\nå¾ {exclude_config_path} è®€å–æ’é™¤æ¸…å–®:")
        print(f"  ç™¼ç¾ {len(exclude_gov_ids)} éƒ¨éœ€è¦æ’é™¤çš„é›»å½±")
        print(f"  æ’é™¤çš„ gov_id: {exclude_gov_ids}")

        # æª¢æŸ¥æœ‰å¤šå°‘ç­†è³‡æ–™æœƒè¢«æ’é™¤
        exclude_count = df[df["gov_id"].isin(exclude_gov_ids)].shape[0]
        exclude_movie_count = df[df["gov_id"].isin(exclude_gov_ids)]["gov_id"].nunique()
        print(f"  å°‡æ’é™¤ {exclude_movie_count} éƒ¨é›»å½±ï¼Œå…± {exclude_count} ç­†è³‡æ–™")

        # åŸ·è¡Œæ’é™¤
        df = df[~df["gov_id"].isin(exclude_gov_ids)].copy()
        print(f"  æ’é™¤å¾Œå‰©é¤˜è³‡æ–™ç­†æ•¸: {len(df)}")
    else:
        print(f"\n{exclude_config_path} ä¸­æ²’æœ‰éœ€è¦æ’é™¤çš„é›»å½±")

except FileNotFoundError:
    print(f"\nè­¦å‘Š: æ‰¾ä¸åˆ°æ’é™¤æ¸…å–®æª”æ¡ˆ {exclude_config_path}ï¼Œè·³éæ’é™¤æ­¥é©Ÿ")
except Exception as e:
    print(f"\nè­¦å‘Š: è®€å–æ’é™¤æ¸…å–®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ï¼Œè·³éæ’é™¤æ­¥é©Ÿ")


# === 2-2. ç¯©é¸è³‡æ–™ ===
# åªä¿ç•™é¦–è¼ªè³‡æ–™
df = df[df["round_idx"] == 1].copy()
# åªä¿ç•™æœ‰æ´»èºé€±æ¬¡çš„è³‡æ–™
df = df[df["current_week_active_idx"].notna()]
# å¿…é ˆåŒæ™‚æœ‰ week_1 å’Œ week_2 çš„è³‡æ–™,ä¸”éƒ½ä¸ç‚º 0
df = df[
    (df["boxoffice_week_1"].notna())
    & (df["boxoffice_week_1"] > 0)
    & (df["boxoffice_week_2"].notna())
    & (df["boxoffice_week_2"] > 0)
]
print(f"åŸºæœ¬ç¯©é¸å¾Œè³‡æ–™ç­†æ•¸: {len(df)}")


# === 3. æœˆä»½é€±æœŸæ€§ç·¨ç¢¼ ===
# ä½¿ç”¨å…±ç”¨ç‰¹å¾µå·¥ç¨‹æ¨¡çµ„é€²è¡Œç·¨ç¢¼
df = BoxOfficeFeatureEngineer.add_features_to_dataframe(df, group_by_col='gov_id')


# === 4. åˆªé™¤ä¸éœ€è¦çš„æ¬„ä½ ===
# å®šç¾©è¦åˆªé™¤çš„æ¬„ä½
drop_columns = [
    # è³‡æ–™æ´©æ¼
    "tickets",
    "theater_count",  # amount è¦ç•™åˆ°æœ€å¾Œæ‰åˆª
    # ä¸éœ€è¦çš„æ™‚é–“è³‡è¨Š
    "official_release_date",
    "week_range",
    "current_week_real_idx",
    # è·¨è¼ªç´¯ç©
    "boxoffice_cumsum",
    "boxoffice_round1_cumsum",
    "boxoffice_current_round_cumsum",  # â† æª¢æŸ¥é€™å€‹
    "audience_cumsum",
    "audience_round1_cumsum",
    "audience_current_round_cumsum",  # â† æª¢æŸ¥é€™å€‹
    "rounds_cumsum",
    # å•é¡Œæ¬„ä½
    "ticket_price_avg_current",
    # åˆ†é¡æ¬„ä½ (æ™‚é–“æœ‰é™å…ˆåˆªé™¤)
    "region",
    "publisher",
    # å·²ç·¨ç¢¼çš„åŸå§‹æ¬„ä½
    "release_month",
]

df = df.drop(columns=drop_columns)


# === 5. æª¢æŸ¥ç¼ºå¤±å€¼ ===
print("\n=== ç¼ºå¤±å€¼æª¢æŸ¥ ===")
print(df.isnull().sum()[df.isnull().sum() > 0])


# === 6. å­˜æª”: å®Œæ•´è³‡æ–™ (å« amount å’Œ gov_id) ===
df.to_csv(output_prepare_dir / "preprocessed_full.csv", index=False, encoding="utf-8-sig")
print(f"\nâœ… å·²å­˜æª”: {output_prepare_dir / 'preprocessed_full.csv'}")
print("ğŸ“è³‡æ–™æ•¸é‡å°è¨ˆ:")
print(f"   æ¬„ä½æ•¸: {len(df.columns)}")
print(f"   è³‡æ–™ç­†æ•¸: {len(df)}")


# === 7. é¡¯ç¤ºæœ€çµ‚æ¬„ä½ ===
print("\n=== ğŸ“æœ€çµ‚æ¬„ä½æ¸…å–® ===")
for i, col in enumerate(df.columns, 1):
    print(f"{i:2d}. {col}")


# === 8. å­˜æª”: è¨“ç·´ç”¨ç‰¹å¾µ (ç§»é™¤ amountï¼Œä¿ç•™ gov_id ç”¨æ–¼åˆ†çµ„) ===
feature_cols = [col for col in df.columns if col != "amount"]
df_features = df[feature_cols]
df_features.to_csv(
    output_prepare_dir / "preprocessed_features.csv", index=False, encoding="utf-8-sig"
)
print(f"\nâœ… å·²å­˜æª”: {output_prepare_dir / 'preprocessed_features.csv'}")


# === 9. å­˜æª”: ç›®æ¨™è®Šæ•¸ ===
df[["gov_id", "amount"]].to_csv(
    output_prepare_dir / "preprocessed_target.csv", index=False, encoding="utf-8-sig"
)
print(f"âœ… å·²å­˜æª”: {output_prepare_dir / 'preprocessed_target.csv'}")


# === 10. çµ±è¨ˆæ‘˜è¦ ===
print("\n=== ğŸ“è³‡æ–™æ‘˜è¦ ===")
print(df[["amount", "boxoffice_week_1", "current_week_active_idx"]].describe())


# ===================================================================
# è¨“ç·´æ¨¡å‹
# ===================================================================
# === 11. åˆ†é›¢ç‰¹å¾µèˆ‡ç›®æ¨™ ===
X = df.drop(columns=["amount"])
y = df["amount"]

print(f"\nç‰¹å¾µçŸ©é™£ X: {X.shape}")
print(f"ç›®æ¨™è®Šæ•¸ y: {y.shape}")

print("\n" + "=" * 50)
print("ğŸ” ç‰¹å¾µèˆ‡ç›®æ¨™ç›¸é—œæ€§ (Top 10)")
print("=" * 50)

correlation = pd.DataFrame(
    {
        "feature": X.drop(columns=["gov_id"]).columns,
        "correlation": X.drop(columns=["gov_id"]).corrwith(y),
    }
).sort_values("correlation", key=abs, ascending=False)

print(correlation.head(10).to_string(index=False))


# === 12. Group-based åˆ‡åˆ†è³‡æ–™é›† ===
from sklearn.model_selection import GroupShuffleSplit

# ç¢ºä¿åŒä¸€éƒ¨é›»å½±çš„æ‰€æœ‰é€±æ¬¡è³‡æ–™ä¸æœƒåŒæ™‚å‡ºç¾åœ¨è¨“ç·´/æ¸¬è©¦é›†
splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)
train_idx, test_idx = next(splitter.split(X, y, groups=X["gov_id"]))

X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

print("\nğŸ“ è¨“ç·´é›†/æ¸¬è©¦é›†åˆ†å‰²çµæœ")
print(f"è¨“ç·´é›†: {len(X_train)} ç­† ({len(X_train['gov_id'].unique())} éƒ¨é›»å½±)")
print(f"æ¸¬è©¦é›†: {len(X_test)} ç­† ({len(X_test['gov_id'].unique())} éƒ¨é›»å½±)")


# === 13. ç§»é™¤ gov_id (åªç”¨æ–¼åˆ†çµ„,ä¸åƒèˆ‡è¨“ç·´) ===
X_train_model = X_train.drop(columns=["gov_id"])
X_test_model = X_test.drop(columns=["gov_id"])

print(f"\næ¨¡å‹è¨“ç·´ç‰¹å¾µæ•¸: {X_train_model.shape[1]}")


# === 14. æª¢æŸ¥ç¼ºå¤±å€¼ ===
print("\n" + "=" * 50)
print("ğŸ” è¨“ç·´é›†ç¼ºå¤±å€¼æª¢æŸ¥")
print("=" * 50)

missing_train = X_train_model.isnull().sum()
missing_train = missing_train[missing_train > 0].sort_values(ascending=False)

if len(missing_train) > 0:
    print("âš ï¸ ç™¼ç¾ç¼ºå¤±å€¼:")
    print(missing_train)
    print(f"\nç¸½ç¼ºå¤±ç­†æ•¸: {X_train_model.isnull().any(axis=1).sum()}/{len(X_train_model)}")
else:
    print("âœ… ç„¡ç¼ºå¤±å€¼")


# === 15. è¨“ç·´æ¨¡å‹: Decision Tree Regressor ===
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

print("\n" + "=" * 50)
print("ğŸŸ¢ æ¨¡å‹: Decision Tree Regressor")
print("=" * 50)

dt_model = DecisionTreeRegressor(
    max_depth=10,  # æ¨¹çš„æœ€å¤§æ·±åº¦
    min_samples_split=20,  # åˆ†è£‚ç¯€é»æ‰€éœ€æœ€å°æ¨£æœ¬æ•¸
    min_samples_leaf=10,  # è‘‰ç¯€é»æ‰€éœ€æœ€å°æ¨£æœ¬æ•¸
    random_state=42,
)

dt_model.fit(X_train_model, y_train)

y_pred_dt = dt_model.predict(X_test_model)

print(f"MAE:  {mean_absolute_error(y_test, y_pred_dt):,.0f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_dt)):,.0f}")
print(f"RÂ²:   {r2_score(y_test, y_pred_dt):.4f}")


# ===================================================================
# æ´å¯Ÿæ¨¡å‹åˆ†æçµæœ
# ===================================================================
# === 16. ç‰¹å¾µé‡è¦æ€§åˆ†æ ===
print("\n" + "=" * 50)
print("ğŸ“Š Top 10 é‡è¦ç‰¹å¾µ (Decision Tree)")
print("=" * 50)

feature_importance = pd.DataFrame(
    {"feature": X_train_model.columns, "importance": dt_model.feature_importances_}
).sort_values("importance", ascending=False)

print(feature_importance.head(10).to_string(index=False))

# å­˜æª”ç‰¹å¾µé‡è¦æ€§
feature_importance.to_csv(
    output_model_dir / "feature_importance.csv", index=False, encoding="utf-8-sig"
)
print(f"\nâœ… ç‰¹å¾µé‡è¦æ€§å·²å­˜æª”: {output_model_dir / 'feature_importance.csv'}")


# === 17. è¦–è¦ºåŒ–: é æ¸¬ vs å¯¦éš› ===
import matplotlib.pyplot as plt
import seaborn as sns

plt.rcParams["font.sans-serif"] = ["Microsoft JhengHei"]  # ä¸­æ–‡å­—å‹
plt.rcParams["axes.unicode_minus"] = False

fig, ax = plt.subplots(figsize=(8, 6))

# Decision Tree
ax.scatter(y_test, y_pred_dt, alpha=0.5, s=10)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--", lw=2)
ax.set_xlabel("å¯¦éš›ç¥¨æˆ¿")
ax.set_ylabel("é æ¸¬ç¥¨æˆ¿")
ax.set_title(f"Decision Tree (RÂ²={r2_score(y_test, y_pred_dt):.3f})")
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(output_model_dir / "prediction_comparison.png", dpi=150, bbox_inches="tight")
print(f"\nâœ… é æ¸¬çµæœåœ–å·²å­˜æª”: {output_model_dir / 'prediction_comparison.png'}")
plt.show()


# === 18. ç‰¹å¾µç›¸é—œæ€§ç†±åŠ›åœ– ===
print("\n" + "=" * 50)
print("ğŸ”¥ ç‰¹å¾µç›¸é—œæ€§ç†±åŠ›åœ–")
print("=" * 50)

# è¨ˆç®—ç›¸é—œæ€§çŸ©é™£ï¼ˆæ’é™¤ gov_idï¼‰
correlation_matrix = X_train_model.corr()

# å»ºç«‹ç†±åŠ›åœ–
plt.figure(figsize=(20, 16))
sns.heatmap(
    correlation_matrix,
    annot=False,  # ç‰¹å¾µå¤ªå¤šæ™‚ä¸é¡¯ç¤ºæ•¸å­—
    cmap="coolwarm",
    center=0,
    square=True,
    linewidths=0.5,
    cbar_kws={"shrink": 0.8},
    vmin=-1,
    vmax=1,
)
plt.title("ç‰¹å¾µç›¸é—œæ€§ç†±åŠ›åœ–", fontsize=16, pad=20)
plt.tight_layout()
plt.savefig(output_model_dir / "correlation_heatmap.png", dpi=150, bbox_inches="tight")
print(f"âœ… ç›¸é—œæ€§ç†±åŠ›åœ–å·²å­˜æª”: {output_model_dir / 'correlation_heatmap.png'}")
plt.show()

# å„²å­˜ç›¸é—œæ€§çŸ©é™£ç‚º CSV
correlation_matrix.to_csv(
    output_model_dir / "correlation_matrix.csv", encoding="utf-8-sig"
)
print(f"âœ… ç›¸é—œæ€§çŸ©é™£å·²å­˜æª”: {output_model_dir / 'correlation_matrix.csv'}")

# æ‰¾å‡ºé«˜åº¦ç›¸é—œçš„ç‰¹å¾µå°ï¼ˆ|r| > 0.8ï¼‰
print("\n" + "=" * 50)
print("âš ï¸  é«˜åº¦ç›¸é—œçš„ç‰¹å¾µå° (|r| > 0.8)")
print("=" * 50)

high_corr_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i + 1, len(correlation_matrix.columns)):
        corr_value = correlation_matrix.iloc[i, j]
        if abs(corr_value) > 0.8:
            high_corr_pairs.append(
                {
                    "feature_1": correlation_matrix.columns[i],
                    "feature_2": correlation_matrix.columns[j],
                    "correlation": corr_value,
                }
            )

if len(high_corr_pairs) > 0:
    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values(
        "correlation", key=abs, ascending=False
    )
    print(high_corr_df.to_string(index=False))
    high_corr_df.to_csv(
        output_model_dir / "high_correlation_pairs.csv", index=False, encoding="utf-8-sig"
    )
    print(f"\nâœ… é«˜ç›¸é—œç‰¹å¾µå°å·²å­˜æª”: {output_model_dir / 'high_correlation_pairs.csv'}")
else:
    print("âœ… æ²’æœ‰ç™¼ç¾é«˜åº¦ç›¸é—œçš„ç‰¹å¾µå°")


# ===================================================================
# å„²å­˜æ¨¡å‹èˆ‡åˆ†æçµæœ
# ===================================================================
# === 19. å„²å­˜æ¨¡å‹ ===
import joblib

joblib.dump((dt_model, X_train_model.columns.tolist()), output_model_dir / "model_decision_tree.pkl")
print(f"\nâœ… æ¨¡å‹å·²å­˜æª”:")
print(f"   - {output_model_dir / 'model_decision_tree.pkl'}")


# === 20. å„²å­˜æ¸¬è©¦é›†é æ¸¬çµæœ ===
results = pd.DataFrame(
    {
        "gov_id": X_test["gov_id"].values,
        "actual": y_test.values,
        "pred_dt": y_pred_dt,
        "error_dt": y_test.values - y_pred_dt,
    }
)

results.to_csv(output_model_dir / "test_predictions.csv", index=False, encoding="utf-8-sig")
print(f"âœ… æ¸¬è©¦é›†é æ¸¬çµæœå·²å­˜æª”: {output_model_dir / 'test_predictions.csv'}")


# === 21. ç´€éŒ„æœ¬æ¬¡åŸ·è¡Œéç¨‹log ===
print("\n" + "=" * 60)
print(f"âœ… è¨“ç·´å®Œæˆ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 60)

# å¯«å…¥ log æª”æ¡ˆ
sys.stdout = sys.stdout.terminal  # æ¢å¾©æ­£å¸¸çš„ stdout
with open(log_file, "w", encoding="utf-8") as f:
    f.write(log_buffer.getvalue())

print(f"\nâœ… è¨“ç·´ç´€éŒ„å·²å­˜æª”: {log_file}")
print("\nğŸ‰ è¨“ç·´å®Œæˆ!")

# ===================================================================
#                                 END
# ===================================================================

# ===================================================================
# è£œå……èªªæ˜
# ===================================================================
"""
## ğŸ“¦ æœ€çµ‚æœƒç”¢ç”Ÿçš„æª”æ¡ˆ
```
data/ML_boxoffice/phase4_models/M3/M3_YYYYMMDD_HHMMSS/
â”œâ”€â”€ prepared_data/
â”‚   â”œâ”€â”€ preprocessed_full.csv           # å®Œæ•´é è™•ç†è³‡æ–™
â”‚   â”œâ”€â”€ preprocessed_features.csv       # ç‰¹å¾µçŸ©é™£ (X)
â”‚   â””â”€â”€ preprocessed_target.csv         # ç›®æ¨™è®Šæ•¸ (y)
â”œâ”€â”€ training_log_YYYYMMDD_HHMMSS.txt    # è¨“ç·´æ—¥èªŒ
â”œâ”€â”€ feature_importance.csv              # ç‰¹å¾µé‡è¦æ€§æ’å
â”œâ”€â”€ prediction_comparison.png           # é æ¸¬ vs å¯¦éš›æ•£ä½ˆåœ–
â”œâ”€â”€ correlation_heatmap.png             # ç‰¹å¾µç›¸é—œæ€§ç†±åŠ›åœ–
â”œâ”€â”€ correlation_matrix.csv              # ç›¸é—œæ€§çŸ©é™£
â”œâ”€â”€ high_correlation_pairs.csv          # é«˜ç›¸é—œç‰¹å¾µå°ï¼ˆå¦‚æœ‰ï¼‰
â”œâ”€â”€ test_predictions.csv                # æ¸¬è©¦é›†è©³ç´°é æ¸¬çµæœ
â””â”€â”€ model_decision_tree.pkl             # å·²è¨“ç·´çš„ Decision Tree æ¨¡å‹
```

## ğŸ” M3 æ¨¡å‹èªªæ˜
- **æ¨¡å‹é¡å‹**: Decision Tree Regressorï¼ˆæ±ºç­–æ¨¹å›æ­¸ï¼‰
- **è¨“ç·´è³‡æ–™**: features_market_2025-11-07.csv
- **æ¨¡å‹åƒæ•¸**:
  - max_depth=10
  - min_samples_split=20
  - min_samples_leaf=10
- **è³‡æ–™è™•ç†**:
  - æ’é™¤æŒ‡å®šé›»å½±
  - åªä¿ç•™é¦–è¼ªè³‡æ–™
  - æœˆä»½é€±æœŸæ€§ç·¨ç¢¼
  - ç§»é™¤è³‡æ–™æ´©æ¼æ¬„ä½
- **è©•ä¼°æŒ‡æ¨™**: MAE, RMSE, RÂ²
"""
