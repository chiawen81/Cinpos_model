# uv run data\ML_rating\train_Model_Rating\crawler_gov_to_imdb_merge_final.py
# ==========================================================
import pandas as pd
import requests
import time
from pathlib import Path

# ==========================================================
# ğŸ”§ åŸºæœ¬è¨­å®š
# ==========================================================
OMDB_API_KEY = "749f9108"   # â† é€™è£¡æ”¾ä½ åœ¨ https://www.omdbapi.com/apikey.aspx çš„é‡‘é‘°
OMDB_URL = "https://www.omdbapi.com/"
INPUT_FILE = r"data\ML_rating\movieInfo_gov_full_2025-11-06.csv"
OUTPUT_FILE = r"data\ML_rating\gov_imdb_full_merge.csv"
CACHE_FILE = r"data\ML_rating\gov_imdb_cache.csv"

# ==========================================================
# ğŸ§  è¼”åŠ©å‡½æ•¸
# ==========================================================
def fetch_imdb_info(title, year=None):
    """é€é OMDb å–å¾— imdb_id èˆ‡ imdb_rating"""
    if not title or pd.isna(title):
        return None, None

    params = {
        "apikey": OMDB_API_KEY,
        "t": title.strip(),
        "type": "movie"
    }
    if year:
        params["y"] = str(year)

    try:
        r = requests.get(OMDB_URL, params=params, timeout=10)
        data = r.json()
        if data.get("Response") == "True":
            return data.get("imdbID"), data.get("imdbRating")
    except Exception as e:
        print(f"âš ï¸  æŸ¥è©¢å¤±æ•—: {title} ({year}) â†’ {e}")

    return None, None


# ==========================================================
# ğŸ“¥ Step 1. è¼‰å…¥é›»å½±æ¸…å–®
# ==========================================================
df = pd.read_csv(INPUT_FILE, dtype=str)
print(f"ğŸ“„ å·²è¼‰å…¥ {len(df)} ç­†é›»å½±è³‡æ–™")

# ==========================================================
# ğŸ’¾ Step 2. è¼‰å…¥å¿«å–ï¼ˆé¿å…é‡æŸ¥ï¼‰
# ==========================================================
cache = {}
if Path(CACHE_FILE).exists():
    df_cache = pd.read_csv(CACHE_FILE, dtype=str)
    cache = {str(r["gov_id"]): (r["imdb_id"], r["imdb_rating"]) for _, r in df_cache.iterrows()}
    print(f"ğŸ’¾ å·²è¼‰å…¥å¿«å– {len(cache)} ç­† IMDb å°æ‡‰è³‡æ–™")

# ==========================================================
# ğŸ”„ Step 3. é€ç­†æŸ¥è©¢ IMDb
# ==========================================================
records = []
for i, row in df.iterrows():
    gov_id = str(row["gov_id"]).strip()
    zh = row.get("gov_title_zh")
    en = row.get("gov_title_en")
    year = None

    imdb_id, imdb_rating = cache.get(gov_id, (None, None))

    if not imdb_id:
        # å„ªå…ˆä»¥è‹±æ–‡ç‰‡åæŸ¥è©¢
        if en:
            imdb_id, imdb_rating = fetch_imdb_info(en, year)
        # è‹¥è‹±æ–‡æŸ¥ä¸åˆ°ï¼Œå‰‡ç”¨ä¸­æ–‡æŸ¥è©¢
        if not imdb_id and zh:
            imdb_id, imdb_rating = fetch_imdb_info(zh, year)

        cache[gov_id] = (imdb_id, imdb_rating)
        print(f"{i+1}/{len(df)} âœ… {gov_id} | {zh or en} â†’ {imdb_id} ({imdb_rating})")
        time.sleep(0.6)  # å»¶é²é¿å…è¢«å°é–
    else:
        print(f"{i+1}/{len(df)} âš¡ å¿«å–å‘½ä¸­: {gov_id} â†’ {imdb_id} ({imdb_rating})")

    # å»ºç«‹åˆä½µç´€éŒ„
    record = row.to_dict()
    record["imdb_id"] = imdb_id
    record["imdb_rating"] = imdb_rating
    records.append(record)

    # æ¯ 20 ç­†æš«å­˜ä¸€æ¬¡
    if (i + 1) % 20 == 0:
        pd.DataFrame(records).to_csv(CACHE_FILE, index=False, encoding="utf-8-sig")
        print(f"ğŸ’¾ æš«å­˜å¿«å–ä¸­... ({i+1} ç­†)")

# ==========================================================
# ğŸ“Š Step 4. åŒ¯å‡ºæœ€çµ‚åˆä½µæª”
# ==========================================================
df_result = pd.DataFrame(records)
df_result.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig")

print("\nâœ… å®Œæˆæ•´åˆï¼å·²è¼¸å‡ºæª”æ¡ˆï¼š", OUTPUT_FILE)
print("ğŸ“ æ¬„ä½åŒ…å«ï¼šgov_id, gov_title_zh, gov_title_en, imdb_id, imdb_rating")
