# uv run src\ML_rating\train_Model_Rating\train_rating_models_addlog.py
# =====================================================
# ğŸ¬ IMDb Rating é›™æ¨¡å‹è¨“ç·´ (LightGBM + RandomForest)
# Version: v2 - åŠ å¼· Logã€ç‰¹å¾µé‡è¦æ€§è¦–è¦ºåŒ–
# =====================================================
import pandas as pd
import numpy as np
import sys
from pathlib import Path
from io import StringIO
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
import lightgbm as lgb
import matplotlib.pyplot as plt
import joblib

# =====================================================
# ğŸ“ è¼¸å‡ºè³‡æ–™å¤¾è¨­å®š
# =====================================================
output_dir = Path("data/ML_rating/2type_rating_models_v2")
output_dir.mkdir(parents=True, exist_ok=True)

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
log_file = output_dir / f"training_log_{timestamp}.txt"
log_buffer = StringIO()

# =====================================================
# ğŸ§¾ Log ç³»çµ±è¨­å®š
# =====================================================
class Logger:
    def __init__(self, log_buffer):
        self.terminal = sys.stdout
        self.log = log_buffer
    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
    def flush(self):
        self.terminal.flush()

sys.stdout = Logger(log_buffer)

def log_model_result(model_name, mae, rmse, r2):
    print(f"\n[{model_name}] è©•ä¼°çµæœ")
    print(f"  MAE : {mae:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  RÂ²  : {r2:.4f}")

def log_top_features(model_name, feature_importances, feature_names, top_n=10):
    print(f"\nğŸ” {model_name} å‰ {top_n} é‡è¦ç‰¹å¾µï¼š")
    top_features = sorted(
        zip(feature_names, feature_importances), key=lambda x: x[1], reverse=True
    )[:top_n]
    for i, (feat, imp) in enumerate(top_features, 1):
        print(f"{i:>2}. {feat:<35} {imp:.3f}")

# =====================================================
# ğŸ“„ è³‡æ–™è®€å–
# =====================================================
feature_path = Path("data/ML_rating/ML_data_PART2/ML_add_extag/merged_full_dataset_full_extag_add_tree_feature.csv")
target_path  = Path("data/ML_rating/ML_data_PART2/ML_add_extag/merged_full_dataset_extag_add_tree_target.csv")

print("=" * 60)
print(f"ğŸ¬ IMDb Rating é›™æ¨¡å‹è¨“ç·´é–‹å§‹ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
print("=" * 60)
print(f"ğŸ“¥ è¼‰å…¥è³‡æ–™ï¼š\nFeature: {feature_path}\nTarget : {target_path}")

X = pd.read_csv(feature_path)
y = pd.read_csv(target_path)

if "gov_id" in X.columns: X = X.drop(columns=["gov_id"])
if "gov_id" in y.columns: y = y.drop(columns=["gov_id"])

# OneHot ç·¨ç¢¼æ–‡å­—æ¬„ä½
cat_cols = X.select_dtypes(include=["object"]).columns.tolist()
if len(cat_cols) > 0:
    print(f"ğŸ”  åµæ¸¬åˆ°æ–‡å­—æ¬„ä½ {len(cat_cols)} å€‹ï¼ŒåŸ·è¡Œ OneHot ç·¨ç¢¼: {cat_cols}")
    X = pd.get_dummies(X, columns=cat_cols, drop_first=True)

# å¡«è£œç¼ºå¤±å€¼
if X.isna().sum().sum() > 0:
    print(f"âš ï¸ X æœ‰ç¼ºå¤±å€¼ï¼Œå·²ä»¥ 0 å¡«è£œ")
    X = X.fillna(0)
if y.isna().sum().sum() > 0:
    print(f"âš ï¸ y æœ‰ç¼ºå¤±å€¼ï¼Œå·²ä»¥ 0 å¡«è£œ")
    y = y.fillna(0)

print(f"âœ… æ¸…ç†å¾Œè³‡æ–™: X={len(X)}, y={len(y)}, ç‰¹å¾µæ•¸={X.shape[1]}")

# =====================================================
# ğŸ”€ åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†
# =====================================================
y = y.iloc[:, 0]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"ğŸ“š è¨“ç·´é›†: {len(X_train)} ç­†, æ¸¬è©¦é›†: {len(X_test)} ç­†")

# =====================================================
# ğŸŸ¢ æ¨¡å‹ 1: LightGBM
# =====================================================
print("\n" + "=" * 50)
print("ğŸŸ¢ æ¨¡å‹ 1: LightGBM")
print("=" * 50)

lgb_model = lgb.LGBMRegressor(
    n_estimators=400, learning_rate=0.05, max_depth=6,
    random_state=42, verbose=-1
)
lgb_model.fit(X_train, y_train)
y_pred_lgb = lgb_model.predict(X_test)

mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))
r2_lgb = r2_score(y_test, y_pred_lgb)
log_model_result("LightGBM", mae_lgb, rmse_lgb, r2_lgb)
log_top_features("LightGBM", lgb_model.feature_importances_, X.columns)

# =====================================================
# ğŸŒ² æ¨¡å‹ 2: Random Forest
# =====================================================
print("\n" + "=" * 50)
print("ğŸŒ² æ¨¡å‹ 2: Random Forest")
print("=" * 50)

rf = RandomForestRegressor(
    n_estimators=400, max_depth=20,
    min_samples_leaf=2, random_state=80, n_jobs=-1
)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

mae_rf = mean_absolute_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)
log_model_result("RandomForest", mae_rf, rmse_rf, r2_rf)
log_top_features("RandomForest", rf.feature_importances_, X.columns)

# =====================================================
# ğŸ“Š ç‰¹å¾µé‡è¦æ€§è¼¸å‡º
# =====================================================
feature_importance = pd.DataFrame({
    "feature": X.columns,
    "LightGBM_Importance": lgb_model.feature_importances_,
    "RandomForest_Importance": rf.feature_importances_
}).sort_values("LightGBM_Importance", ascending=False)

feature_importance.to_csv(output_dir / "feature_importance_2models.csv", index=False, encoding="utf-8-sig")

top_features = feature_importance.head(15)
plt.rcParams["font.sans-serif"] = ["Microsoft JhengHei"]
plt.rcParams["axes.unicode_minus"] = False
plt.figure(figsize=(10,6))
plt.barh(top_features["feature"], top_features["LightGBM_Importance"], color='orange')
plt.gca().invert_yaxis()
plt.title("LightGBM æœ€é‡è¦çš„ 15 å€‹ç‰¹å¾µ")
plt.xlabel("Feature Importance")
plt.tight_layout()
plt.savefig(output_dir / "feature_importance_top15.png", dpi=150)
plt.close()

# =====================================================
# ğŸ“ˆ é æ¸¬æ¯”è¼ƒåœ–
# =====================================================
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
models = [("LightGBM", y_pred_lgb, r2_lgb), ("RandomForest", y_pred_rf, r2_rf)]
for ax, (name, pred, r2) in zip(axes, models):
    ax.scatter(y_test, pred, alpha=0.6, s=20)
    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--")
    ax.set_title(f"{name} (RÂ²={r2:.3f})")
    ax.set_xlabel("å¯¦éš› IMDb è©•åˆ†")
    ax.set_ylabel("é æ¸¬ IMDb è©•åˆ†")
    ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(output_dir / "rating_prediction_2models.png", dpi=150)
plt.close()

# =====================================================
# ğŸ’¾ æ¨¡å‹ä¿å­˜èˆ‡æ¸¬è©¦è¼¸å‡º
# =====================================================
joblib.dump(lgb_model, output_dir / "model_lightgbm.pkl")
joblib.dump(rf, output_dir / "model_randomforest.pkl")
results = pd.DataFrame({
    "actual": y_test,
    "pred_lightgbm": y_pred_lgb,
    "pred_randomforest": y_pred_rf
})
results.to_csv(output_dir / "test_predictions_2models.csv", index=False, encoding="utf-8-sig")

# =====================================================
# ğŸ“˜ è¨“ç·´çµæœæ‘˜è¦
# =====================================================
print("\n" + "=" * 60)
print("ğŸ“˜ æ¨¡å‹è¨“ç·´æ‘˜è¦")
print("=" * 60)
print(f"LightGBM â†’ RÂ²={r2_lgb:.4f}, RMSE={rmse_lgb:.4f}")
print(f"RandomForest â†’ RÂ²={r2_rf:.4f}, RMSE={rmse_rf:.4f}")
print(f"\nğŸ“‚ æ¨¡å‹è¼¸å‡ºè³‡æ–™å¤¾: {output_dir}")
print(f"ğŸ§¾ Log æª”æ¡ˆ: {log_file.name}")
print(f"ğŸ“Š ç‰¹å¾µé‡è¦æ€§: feature_importance_2models.csv")
print(f"ğŸ“ˆ åœ–åƒè¼¸å‡º: rating_prediction_2models.png")
print(f"ğŸ“Š Top15 ç‰¹å¾µåœ–: feature_importance_top15.png")
print(f"ğŸ“‘ æ¸¬è©¦çµæœ: test_predictions_2models.csv")
print("=" * 60)

# =====================================================
# ğŸ§¾ Log è¼¸å‡º
# =====================================================
sys.stdout = sys.stdout.terminal
with open(log_file, "w", encoding="utf-8") as f:
    f.write(log_buffer.getvalue())
print(f"ğŸ‰ æ¨¡å‹è¨“ç·´å®Œæˆï¼Log å·²ä¿å­˜è‡³: {log_file}")