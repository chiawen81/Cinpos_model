"""
æ”¿åºœå…¬é–‹è³‡æ–™ï¼šã€Šå…¨åœ‹é›»å½±ç¥¨æˆ¿çµ±è¨ˆè³‡è¨Šã€‹çˆ¬èŸ²
------------------------------------------------
ç›®æ¨™ï¼š
    ä»¥é–‹çœ¼é›»å½±ç¶²çš„é¦–è¼ªé›»å½±åå–®ï¼ˆå­˜åœ¨ Cinpos_model/dataï¼‰ç‚ºåŸºæº–ï¼Œ
    å‰å¾€ã€Šå…¨åœ‹é›»å½±ç¥¨æˆ¿çµ±è¨ˆè³‡è¨Šã€‹æŸ¥è©¢è©²é›»å½±çš„ç¥¨æˆ¿çµ±è¨ˆè³‡æ–™ã€‚

è³‡æ–™ä¾†æºï¼š
    1. é¦–è¼ªé›»å½±åå–®ï¼šé–‹çœ¼é›»å½±ç¶²
                   ï¼ˆå·²å­˜åœ¨æ–¼ data\processed\firstRunFilm_listï¼‰
    2. æ”¿åºœé›»å½±ç¥¨æˆ¿çµ±è¨ˆï¼š
        (1) æœå°‹é›»å½± IDï¼š
            https://boxofficetw.tfai.org.tw/film/sf?keyword=<é›»å½±å>
        (2) å–å¾—ç¥¨æˆ¿è³‡æ–™ï¼š
            https://boxofficetw.tfai.org.tw/film/gfd/<é›»å½±id>
"""

# ========= å¥—ä»¶åŒ¯å…¥ =========
import os
import re
import time
import json
import requests
import pandas as pd
from urllib.parse import quote

# å…±ç”¨æ¨¡çµ„
from common.path_utils import FIRSTRUN_PROCESSED, BOXOFFICE_PERMOVIE_RAW
from common.network_utils import get_default_headers
from common.file_utils import ensure_dir, save_json
from common.date_utils import get_current_week_label


# ========= å…¨åŸŸè¨­å®š =========
SEARCH_URL = "https://boxofficetw.tfai.org.tw/film/sf?keyword="
DETAIL_URL = "https://boxofficetw.tfai.org.tw/film/gfd/"
HEADERS = get_default_headers()
TIMEOUT = 10
SLEEP_INTERVAL = 1.2  # é¿å…é€£çºŒè«‹æ±‚éå¿«è¢«é™åˆ¶


# ========= è¼”åŠ©å‡½å¼ =========
# æŠ“é›»å½± ID
def search_film_id(keyword: str) -> str | None:
    """æ ¹æ“šé›»å½±åç¨±æŸ¥è©¢æ”¿åºœè³‡æ–™åº«ä¸­çš„é›»å½± ID"""
    try:
        res = requests.get(SEARCH_URL + quote(keyword), headers=HEADERS, timeout=TIMEOUT)
        res.encoding = "utf-8"
        data = res.json()
        print(f"ğŸ” æŸ¥è©¢çµæœ(è³‡æ–™)ï¼š{data}")

        try:
            film_id = data["data"]["results"][0]["movieId"]
            print(f"ğŸ” æŸ¥è©¢çµæœ(é›»å½±ID)ï¼š{film_id}")
            return film_id
        except (KeyError, TypeError, IndexError):
            return None

    except Exception as e:
        print(f"âŒ æŸ¥è©¢IDå¤±æ•—ï¼š{keyword} ({e})")
        return None


# æŠ“ç¥¨æˆ¿è³‡æ–™
def fetch_boxoffice_data(film_id: str) -> dict | None:
    """æ ¹æ“šé›»å½± ID æŠ“å–ç¥¨æˆ¿çµ±è¨ˆè³‡æ–™"""
    try:
        res = requests.get(DETAIL_URL + film_id, headers=HEADERS, timeout=TIMEOUT)
        res.encoding = "utf-8"
        data = res.json()
        return data
    except Exception as e:
        print(f"âŒ ç¥¨æˆ¿è³‡æ–™æŠ“å–å¤±æ•—ï¼šID={film_id} ({e})")
        return None


# å„²å­˜ CSV
def sanitize_filename(name: str) -> str:
    """ç§»é™¤æª”åä¸­ä¸åˆæ³•çš„å­—å…ƒ"""
    return re.sub(r'[\\/*?:"<>|]', "_", name)


# å°‡æœªæ‰¾åˆ° ID çš„é›»å½±è³‡æ–™å„²å­˜ç‚º error_{week_label}.json
def save_missing_rows(missing_rows: list[dict], output_dir: str, week_label: str) -> None:
    """å°‡æœªæ‰¾åˆ°é›»å½± ID çš„è³‡æ–™å„²å­˜ç‚º error_{week_label}.json"""
    if missing_rows:
        fileName = f"error_{week_label}.json"
        save_json(missing_rows, output_dir, fileName)
        filePath = os.path.join(output_dir, fileName)
        print(f"âš ï¸ å·²å„²å­˜ {len(missing_rows)} ç­†æœªæ‰¾åˆ°é›»å½± ID/ç¥¨æˆ¿è³‡æ–™ï¼š{filePath}")


# è¨˜éŒ„éŒ¯èª¤é¡åˆ¥
def mark_errorType(row: pd.Series, errorType: str) -> dict:
    row_dict = row.to_dict()
    row_dict["errorType"] = errorType
    print(f"âš ï¸ æœªæ‰¾åˆ°é›»å½± IDï¼š{row['title_zh']}")
    return row_dict


# ========= ä¸»çˆ¬èŸ²é‚è¼¯ =========
### å–å¾—æ”¿åºœå…¬é–‹çš„ç¥¨æˆ¿è³‡æ–™
def fetch_boxoffice_permovie() -> None:
    week_label = get_current_week_label()
    firstRunList_filePath = f"{FIRSTRUN_PROCESSED}\\{week_label}\\firstRun_{week_label}.csv"
    output_dir = os.path.join(BOXOFFICE_PERMOVIE_RAW, week_label)
    missing_rows: list[dict] = []  # ç”¨ä¾†æ”¶é›†æœªæ‰¾åˆ°é›»å½± ID çš„æ•´åˆ—è³‡æ–™
    ensure_dir(output_dir)

    # è®€å–é¦–è¼ªé›»å½±åå–®
    if not os.path.exists(firstRunList_filePath):
        print(f"âš ï¸ æ‰¾ä¸åˆ°æœ¬é€±é¦–è¼ªé›»å½±æ¸…å–®ï¼š{firstRunList_filePath}")
        return

    df_movies = pd.read_csv(firstRunList_filePath)
    print(f"ğŸ“‹ å…± {len(df_movies)} éƒ¨é›»å½±å¾…è™•ç†\n")

    # é€éƒ¨æ•´ç†é›»å½±è³‡æ–™
    for _, row in df_movies.iterrows():
        title = row["title_zh"]
        safe_title = sanitize_filename(title)
        print(f"ğŸ¬ è™•ç†ä¸­ï¼š{title}")

        # Step 1: æŸ¥é›»å½± ID
        film_id = search_film_id(title)
        # å°‡æœªæ‰¾åˆ°IDçš„è³‡æ–™åŠ å…¥ missing_rows
        if not film_id:
            missing_rows.append(mark_errorType(row, "notFoundID"))
            continue

        # Step 2: æŠ“ç¥¨æˆ¿è³‡æ–™
        data = fetch_boxoffice_data(film_id)
        # å°‡æœªæ‰¾åˆ°IDçš„è³‡æ–™åŠ å…¥ missing_rows
        if not data:
            missing_rows.append(mark_errorType(row, "notFoundData"))
            continue

        # Step 3: å„²å­˜ JSONï¼ˆæ¯éƒ¨é›»å½±ä¸€æª”ï¼‰
        file_name = f"{film_id}_{safe_title}_{week_label}.json"
        save_json(data, output_dir, file_name)

        print(f"âœ… å·²å„²å­˜ï¼š{file_name}")
        time.sleep(SLEEP_INTERVAL)

    # å°‡æœªæ‰¾åˆ° ID çš„é›»å½±è³‡æ–™å„²å­˜ç‚º error_{week_label}.json
    save_missing_rows(missing_rows, output_dir, week_label)

    print("\nğŸ‰ æ”¿åºœç¥¨æˆ¿è³‡æ–™çˆ¬å–å®Œæˆï¼")


# ========= ä¸»ç¨‹å¼åŸ·è¡Œå€ =========
if __name__ == "__main__":
    fetch_boxoffice_permovie()
