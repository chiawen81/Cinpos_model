"""
æ”¿åºœå…¬é–‹è³‡æ–™ï¼šã€Šå…¨åœ‹é›»å½±ç¥¨æˆ¿çµ±è¨ˆè³‡è¨Šã€‹çˆ¬èŸ²
------------------------------------------------
ç›®æ¨™ï¼š
    ä»¥é–‹çœ¼é›»å½±ç¶²çš„é¦–è¼ªé›»å½±åå–®ï¼ˆå­˜åœ¨ Cinpos_model/dataï¼‰ç‚ºåŸºæº–ï¼Œ
    å‰å¾€ã€Šå…¨åœ‹é›»å½±ç¥¨æˆ¿çµ±è¨ˆè³‡è¨Šã€‹æŸ¥è©¢è©²é›»å½±çš„ç¥¨æˆ¿çµ±è¨ˆè³‡æ–™ã€‚

è³‡æ–™ä¾†æºï¼š
    1. é¦–è¼ªé›»å½±åå–®ï¼šé–‹çœ¼é›»å½±ç¶²
                    ï¼ˆå·²å­˜åœ¨æ–¼ data\processed\firstRunFilm_listï¼‰
    2. æ”¿åºœé›»å½±ç¥¨æˆ¿çµ±è¨ˆï¼š
        (1) æœå°‹é›»å½± IDï¼š
            https://boxofficetw.tfai.org.tw/film/sf?keyword=<é›»å½±å>
        (2) å–å¾—ç¥¨æˆ¿è³‡æ–™ï¼š
            https://boxofficetw.tfai.org.tw/film/gfd/<é›»å½±id>
"""

# ========= å¥—ä»¶åŒ¯å…¥ =========
import os
import re
import time
import unicodedata
import requests
import pandas as pd
from urllib.parse import quote
from datetime import datetime
from rapidfuzz import fuzz
import math

# å…±ç”¨æ¨¡çµ„
from common.path_utils import FIRSTRUN_PROCESSED, BOXOFFICE_PERMOVIE_RAW
from common.network_utils import get_default_headers
from common.file_utils import ensure_dir, save_json, clean_filename
from common.date_utils import get_current_week_label
from common.mapping_utils import load_manual_mapping, find_manual_mapping

# ========= å…¨åŸŸè¨­å®š =========
SEARCH_URL = "https://boxofficetw.tfai.org.tw/film/sf?keyword="
DETAIL_URL = "https://boxofficetw.tfai.org.tw/film/gfd/"
HEADERS = get_default_headers()
TIMEOUT = 10
SLEEP_INTERVAL = 1.2  # é¿å…é€£çºŒè«‹æ±‚éå¿«è¢«é™åˆ¶
manual_mappings = load_manual_mapping()


# ========= è¼”åŠ©å‡½å¼ =========
def normalize_title(title: str) -> str:
    """æ­£è¦åŒ–é›»å½±åç¨±ï¼Œæå‡æ¨¡ç³Šæ¯”å°æº–ç¢ºç‡"""
    if not title:
        return ""
    title = unicodedata.normalize("NFKC", title)
    title = re.sub(
        r"[ï¼š:ã€ï¼Œ,ï¼ã€‚ï¼Ÿï¼!ï¼Ÿï½ï½â€ï¼â€”â€“â€§Â·â€¢ã€ã€ã€Œã€()ï¼ˆï¼‰ã€Šã€‹ã€ˆã€‰ã€ã€‘\[\]{}â€¦\s]", "", title
    )
    return title.lower()


def simplify_keyword(keyword: str) -> str:
    """ç°¡åŒ–æœå°‹é—œéµè©ï¼šç§»é™¤å†’è™Ÿã€ç ´æŠ˜è™Ÿå¾Œå–å‰åŠæ®µ"""
    keyword = re.split(r"[ï¼š:ï¼â€”â€“\-]", keyword)[0]
    return keyword.strip()


def compute_similarity(a: str, b: str) -> float:
    """å­—ä¸²ç›¸ä¼¼åº¦"""
    a_n, b_n = normalize_title(a), normalize_title(b)
    # ç¶œåˆä¸‰ç¨®æ¼”ç®—æ³•åˆ†æ•¸
    score_1 = fuzz.ratio(a_n, b_n)
    score_2 = fuzz.partial_ratio(a_n, b_n)
    score_3 = fuzz.token_sort_ratio(a, b)  # ä¿ç•™åŸå§‹ç©ºç™½æ¯”å°
    return max(score_1, score_2, score_3) / 100


def compute_date_similarity(target_date: str, candidate_date: str) -> float:
    """æ ¹æ“šä¸Šæ˜ æ—¥æœŸç›¸è¿‘ç¨‹åº¦çµ¦åˆ†ï¼ˆå·®è·è¶Šå¤§åˆ†æ•¸è¶Šä½ï¼‰"""
    try:
        d1 = datetime.strptime(target_date.replace("/", "-"), "%Y-%m-%d")
        d2 = datetime.strptime(candidate_date.replace("/", "-"), "%Y-%m-%d")
        delta_days = abs((d1 - d2).days)
        return max(0, 1 - delta_days / 365)  # å·®1å¹´ = 0åˆ†
    except Exception:
        return 0.5  # ç„¡æ³•æ¯”å°æ™‚çµ¦ä¸­é–“å€¼


def search_film_id(keyword: str, release_date: str = None) -> str | None:
    """æ ¹æ“šé›»å½±åç¨±èˆ‡ä¸Šæ˜ æ—¥æœŸï¼Œæœå°‹æœ€ç›¸è¿‘çš„é›»å½± ID"""

    def _fetch_results(kw: str):
        """ç™¼é€æœå°‹è«‹æ±‚ä¸¦å›å‚³çµæœåˆ—è¡¨"""
        try:
            res = requests.get(SEARCH_URL + quote(kw), headers=HEADERS, timeout=TIMEOUT)
            res.encoding = "utf-8"
            return res.json().get("data", {}).get("results", [])
        except Exception as e:
            print(f"âŒ æŸ¥è©¢å¤±æ•—ï¼š{kw} ({e})")
            return []

    # ---- ç¬¬ä¸€è¼ªæœå°‹ï¼ˆåŸå§‹æ¨™é¡Œï¼‰----
    results = _fetch_results(keyword)

    # ---- ç¬¬äºŒè¼ªæœå°‹ï¼ˆç°¡åŒ–é—œéµå­—ï¼‰----
    if not results:
        simple_kw = simplify_keyword(keyword)
        if simple_kw != keyword:
            print(f"âš™ï¸ é™ç´šæœå°‹ï¼š{keyword} â†’ {simple_kw}")
            results = _fetch_results(simple_kw)

    if not results:
        print(f"âŒ æŸ¥ç„¡çµæœï¼š{keyword}")
        return None

    # ---- æ¨¡ç³Šæ¯”å°æŒ‘æœ€åƒçš„ ----
    best_match, best_score = None, 0
    scores = []

    for r in results:
        name = r.get("name", "")
        norm_a = normalize_title(keyword)
        norm_b = normalize_title(name)

        # --- 1ï¸âƒ£ åŒ…å«åˆ¤å®šï¼ˆå¼·åŒ¹é…ï¼‰---
        if norm_a in norm_b or norm_b in norm_a:
            score = 1.0  # å®Œå…¨åŒ…å«è¦–ç‚ºæ»¿åˆ†
        else:
            # --- 2ï¸âƒ£ æ¨¡ç³Šæ¯”å° ---
            score = fuzz.token_sort_ratio(keyword, name) / 100.0
            # --- æ—¥æœŸåŠ æ¬Š ---
            if release_date and r.get("releaseDate"):
                date_score = compute_date_similarity(release_date, r["releaseDate"])
                score = 0.9 * score + 0.1 * date_score  # æ—¥æœŸä½” 10%

        scores.append((r, score))

    # --- 3ï¸âƒ£ å‹•æ…‹æ±ºç­– ---
    scores.sort(key=lambda x: x[1], reverse=True)
    if not scores:
        print(f"âŒ ç„¡æœå°‹çµæœï¼š{keyword}")
        return None

    best_match, best_score = scores[0]
    diff = best_score - scores[1][1] if len(scores) > 1 else best_score

    # åˆ¤å®šæ¢ä»¶ï¼šé«˜åˆ†æˆ–æ˜é¡¯é ˜å…ˆ
    if best_score >= 0.7 or diff >= 0.15:
        print(f"âœ… {keyword} â†’ {best_match['name']} (score={best_score:.2f})")
        return best_match["movieId"]
    else:
        print(f"âš ï¸ ç„¡æ˜ç¢ºåŒ¹é…ï¼š{keyword}ï¼ˆæœ€é«˜åˆ† {best_score:.2f}ï¼‰")
        print("å€™é¸ï¼š", [(r["name"], round(s, 2)) for r, s in scores[:3]])
        return None


# æŠ“ç¥¨æˆ¿è³‡æ–™
def fetch_boxoffice_data(film_id: str) -> dict | None:
    """æ ¹æ“šé›»å½± ID æŠ“å–ç¥¨æˆ¿çµ±è¨ˆè³‡æ–™"""
    try:
        res = requests.get(DETAIL_URL + film_id, headers=HEADERS, timeout=TIMEOUT)
        res.encoding = "utf-8"
        data = res.json()
        return data
    except Exception as e:
        print(f"âŒ ç¥¨æˆ¿è³‡æ–™æŠ“å–å¤±æ•—ï¼šID={film_id} ({e})")
        return None


# å°‡æœªæ‰¾åˆ° ID çš„é›»å½±è³‡æ–™å„²å­˜ç‚º error_{week_label}.json
def save_missing_rows(missing_rows: list[dict], output_dir: str, week_label: str) -> None:
    """å°‡æœªæ‰¾åˆ°é›»å½± ID çš„è³‡æ–™å„²å­˜ç‚º error_{week_label}.json"""
    if missing_rows:
        error_dir = os.path.join(output_dir, "error")
        ensure_dir(error_dir)

        fileName = f"error_{week_label}.json"
        save_json(missing_rows, error_dir, fileName)

        print(
            f"âš ï¸ å·²å„²å­˜ {len(missing_rows)} ç­†æœªæ‰¾åˆ°é›»å½± ID/ç¥¨æˆ¿è³‡æ–™ï¼š{os.path.join(error_dir, fileName)}"
        )


# è¨˜éŒ„éŒ¯èª¤é¡åˆ¥
def mark_errorType(row: pd.Series, errorType: str) -> dict:
    row_dict = row.to_dict()
    clean_dict = {
        k: (None if (isinstance(v, float) and math.isnan(v)) else v) for k, v in row_dict.items()
    }
    clean_dict["errorType"] = errorType
    errMsg = "æœªæ‰¾åˆ°é›»å½± ID" if errorType == "notFoundID" else "æœªæ‰¾åˆ°ç¥¨æˆ¿è³‡æ–™"
    print(f"âš ï¸ {errMsg}ï¼š{row.get('title_zh', 'æœªçŸ¥æ¨™é¡Œ')}")
    return clean_dict


# ========= ä¸»çˆ¬èŸ²é‚è¼¯ =========
### å–å¾—æ”¿åºœå…¬é–‹çš„ç¥¨æˆ¿è³‡æ–™
def fetch_boxoffice_permovie() -> None:
    week_label = get_current_week_label()
    firstRunList_filePath = f"{FIRSTRUN_PROCESSED}\\{week_label}\\firstRun_{week_label}.csv"
    output_dir = os.path.join(BOXOFFICE_PERMOVIE_RAW, week_label)
    missing_rows: list[dict] = []  # ç”¨ä¾†æ”¶é›†æœªæ‰¾åˆ°é›»å½± ID çš„æ•´åˆ—è³‡æ–™
    ensure_dir(output_dir)

    # è®€å–é¦–è¼ªé›»å½±åå–®
    if not os.path.exists(firstRunList_filePath):
        print(f"âš ï¸ æ‰¾ä¸åˆ°æœ¬é€±é¦–è¼ªé›»å½±æ¸…å–®ï¼š{firstRunList_filePath}")
        return

    df_movies = pd.read_csv(firstRunList_filePath)
    print(f"ğŸ“‹ å…± {len(df_movies)} éƒ¨é›»å½±å¾…è™•ç†\n")

    # é€éƒ¨æ•´ç†é›»å½±è³‡æ–™
    for _, row in df_movies.iterrows():
        title = row["title_zh"]
        id=row["atmovies_id"]
        safe_title = clean_filename(title)
        release_date = row.get("release_date", "")
        print(f"ğŸ¬ è™•ç†ä¸­ï¼š{title},{id}")

        # Step 1ï¸âƒ£ï¼šå…ˆæª¢æŸ¥äººå·¥å°ç…§è¡¨
        mapping = find_manual_mapping(title, manual_mappings)
        if mapping:
            film_id = mapping.get("gov_id")
            print(f"ğŸ§­ ä½¿ç”¨äººå·¥å°ç…§ï¼š{title} â†’ {mapping['gov_title_zh']} (ID={film_id})")
        else:
            # Step 2ï¸âƒ£ï¼šæ­£å¸¸æœå°‹
            film_id = search_film_id(title, release_date)

        # å°‡æœªæ‰¾åˆ°IDçš„è³‡æ–™åŠ å…¥ missing_rows
        if not film_id:
            missing_rows.append(mark_errorType(row, "notFoundID"))
            continue

        # Step 3ï¸âƒ£ï¼šæŠ“ç¥¨æˆ¿è³‡æ–™
        data = fetch_boxoffice_data(film_id)
        # å°‡æœªæ‰¾åˆ°IDçš„è³‡æ–™åŠ å…¥ missing_rows
        if not data:
            missing_rows.append(mark_errorType(row, "notFoundData"))
            continue

        # Step 3: å„²å­˜ JSONï¼ˆæ¯éƒ¨é›»å½±ä¸€æª”ï¼‰
        file_name = f"{film_id}_{safe_title}_{week_label}_{row["atmovies_id"]}.json"
        save_json(data, output_dir, file_name)

        print(f"âœ… å·²å„²å­˜ï¼š{file_name}")
        time.sleep(SLEEP_INTERVAL)

    # å°‡æœªæ‰¾åˆ° ID çš„é›»å½±è³‡æ–™å„²å­˜ç‚º error_{week_label}.json
    save_missing_rows(missing_rows, output_dir, week_label)

    print("\nğŸ‰ æ”¿åºœç¥¨æˆ¿è³‡æ–™çˆ¬å–å®Œæˆï¼")


# ========= ä¸»ç¨‹å¼åŸ·è¡Œå€ =========
if __name__ == "__main__":
    fetch_boxoffice_permovie()
