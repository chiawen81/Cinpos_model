"""
é–‹çœ¼é›»å½±ç¶²ï¼šé¦–è¼ªä¸Šæ˜ é›»å½±æ¸…å–®çˆ¬èŸ²
----------------------------------
ç›®æ¨™ï¼šå–å¾—é–‹çœ¼é›»å½±ç¶²çš„ã€Œé¦–è¼ªä¸Šæ˜ ä¸­ã€é›»å½±æ¸…å–®
æ›´æ–°é »ç‡ï¼šæ¯å‘¨ä¸€æ¬¡
è³‡æ–™ä¾†æºï¼šhttps://www.atmovies.com.tw/movie/now
"""

# ========= å¥—ä»¶åŒ¯å…¥ =========
import re
import requests
from bs4 import BeautifulSoup

# å…±ç”¨æ¨¡çµ„
from common.date_utils import get_current_week_label
from common.network_utils import get_default_headers
from common.file_utils import save_json
from common.path_utils import FIRSTRUN_RAW

# ========= å…¨åŸŸè¨­å®š =========
BASE_URL = "https://www.atmovies.com.tw"  # é–‹çœ¼é›»å½±ç¶²ç¶²å€
TARGET_URL = f"{BASE_URL}/movie/now/"  # é–‹çœ¼é›»å½±ç¶²çš„é¦–è¼ªé›»å½±é é¢è·¯å¾‘
HEADERS = get_default_headers()


# ========= è¼”åŠ©å‡½å¼ =========
# æ•´ç†å–®ç­†é›»å½±è³‡æ–™
def format_crawler_data(li):
    # å–å¾—ä¸­æ–‡ç‰‡å
    title_tag = li.find("a")
    title = title_tag.text.strip() if title_tag else None

    # å–å¾—é–‹çœ¼é›»å½±ç·¨è™Ÿ
    href = title_tag["href"] if title_tag else ""
    movie_id_match = re.search(r"/movie/([a-z0-9]+)/", href)
    movie_id = movie_id_match.group(1) if movie_id_match else None

    # å–å¾—ä¸Šæ˜ æ—¥æœŸ & ç‰‡é•· & å»³æ•¸
    runtime_span = li.find("span", class_="runtime")
    date_match = re.search(r"(\d{4}/\d{1,2}/\d{1,2})", runtime_span.text)
    theater_match = re.search(r"\((\d+)å»³\)", runtime_span.text)

    release_date = date_match.group(1) if date_match else None
    theater_count = int(theater_match.group(1)) if theater_match else None

    return {
        "atmovies_id": movie_id,
        "title_zh": title,
        "release_date": release_date,
        "screen_count": theater_count,
        "source_url": f"{BASE_URL}/movie/{movie_id}/",
    }


# ========= ä¸»çˆ¬èŸ²é‚è¼¯ =========
### æŠ“å–é–‹çœ¼é›»å½±ç¶²ã€Œé¦–è¼ªä¸Šæ˜ ä¸­ã€æ¸…å–®
def fetch_first_run_movies():
    print("ğŸ“¡ æ­£åœ¨æŠ“å–é–‹çœ¼é›»å½±ç¶²é¦–è¼ªé›»å½±æ¸…å–®...")
    res = requests.get(TARGET_URL, headers=HEADERS, timeout=10)
    res.encoding = "utf-8"
    soup = BeautifulSoup(res.text, "html.parser")
    film_list = soup.find(attrs={"class": "filmListPA"})
    movies = []

    # æª¢æŸ¥æœ‰æ‰¾åˆ°è³‡æ–™
    if not film_list:
        print("âŒ æ‰¾ä¸åˆ°é›»å½±æ¸…å–®å€å¡Š (filmListPA)")
        return []

    # æ•´ç†çˆ¬èŸ²è³‡æ–™
    for li in film_list.find_all("li", recursive=False):
        managed_data = format_crawler_data(li)
        movies.append(managed_data)

    print(f"âœ… å…±æŠ“å–åˆ° {len(movies)} éƒ¨é›»å½±")
    return movies


# ========= å„²å­˜é‚è¼¯ =========
# å„²å­˜åŸå§‹ JSON æª”
def save_raw_json(movies):
    week_label = get_current_week_label()
    json_filename = f"firstRun_{week_label}.json"
    save_json(movies, FIRSTRUN_RAW, json_filename)
    print(f"ğŸ’¾ å·²å„²å­˜ JSONï¼šdata/raw/firstRunFilm_list/{json_filename}")


# ========= ä¸»ç¨‹å¼åŸ·è¡Œå€ =========
if __name__ == "__main__":
    # å–å¾—é›»å½±è³‡æ–™
    movies = fetch_first_run_movies()

    # å„²å­˜è³‡æ–™
    if movies:
        save_raw_json(movies)
        print("ğŸ‰ é–‹çœ¼é›»å½±ç¶²é¦–è¼ªæ¸…å–®çˆ¬å–å®Œæˆï¼")
